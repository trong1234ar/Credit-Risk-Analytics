{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, LabelEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "import datetime as dt\n",
    "import toad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-click Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_model = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = df.select_dtypes(include='object').columns.tolist()\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_fill_missing_app(dataframe):\n",
    "    # convert days to years\n",
    "    def days_to_year(dataframe):\n",
    "        for col in dataframe.columns[dataframe.columns.str.startswith('DAYS')]:\n",
    "            dataframe[f'YEARS{col[4:]}'] = np.ceil(dataframe[col] / -365)\n",
    "        return dataframe\n",
    "    dataframe = days_to_year(dataframe)\n",
    "    # delete invalid in years (< 0)\n",
    "    dataframe[['YEARS_BIRTH','YEARS_EMPLOYED','YEARS_REGISTRATION','YEARS_ID_PUBLISH', 'YEARS_LAST_PHONE_CHANGE']] = dataframe[['YEARS_BIRTH','YEARS_EMPLOYED','YEARS_REGISTRATION','YEARS_ID_PUBLISH', 'YEARS_LAST_PHONE_CHANGE']].applymap(lambda x: np.nan if x < 0 else x)\n",
    "    # delete invalid in days (> 0)\n",
    "    dataframe[['DAYS_BIRTH','DAYS_EMPLOYED','DAYS_REGISTRATION','DAYS_ID_PUBLISH', 'DAYS_LAST_PHONE_CHANGE']] = dataframe[['DAYS_BIRTH','DAYS_EMPLOYED','DAYS_REGISTRATION','DAYS_ID_PUBLISH', 'DAYS_LAST_PHONE_CHANGE']].applymap(lambda x: np.nan if x > 0 else x)\n",
    "\n",
    "    # delete invalid in gender\n",
    "    dataframe['CODE_GENDER'] = np.where(dataframe['CODE_GENDER'] == 'XNA', np.nan, dataframe['CODE_GENDER'])\n",
    "\n",
    "\n",
    "    # drop useless columns\n",
    "    avg_col = list(dataframe.columns[dataframe.columns.str.endswith('AVG')])\n",
    "    mode_col = list(dataframe.columns[dataframe.columns.str.endswith('MODE')])\n",
    "    med_col = list(dataframe.columns[dataframe.columns.str.endswith('MEDI')])\n",
    "    addition_drop = []\n",
    "    col_to_drop = avg_col + mode_col + med_col + addition_drop\n",
    "\n",
    "    # return col_to_drop\n",
    "    # dataframe.drop(columns=col_to_drop, inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "# def lazy_fill_missing_app(dataframe_train, dataframe_test):\n",
    "#     col_num = dataframe_train.select_dtypes('number').columns.drop('TARGET')\n",
    "#     for col in col_num:\n",
    "#         median_value = dataframe_train[col].median()\n",
    "#         dataframe_train[col] = dataframe_train[col].fillna(median_value)\n",
    "#         dataframe_test[col] = dataframe_test[col].fillna(median_value)\n",
    "#     col_object = dataframe_train.select_dtypes('object').columns\n",
    "#     for col in col_object:\n",
    "#         mode_value = dataframe_train[col].mode()[0]\n",
    "#         dataframe_train[col] = dataframe_train[col].fillna(mode_value)\n",
    "#         dataframe_test[col] = dataframe_test[col].fillna(mode_value)\n",
    "#     # dataframe_train['AGE_BIN'] = pd.cut(dataframe_train['YEARS_BIRTH'], 10)\n",
    "#     # # dataframe_test['AGE_BIN'] = pd.cut(dataframe_test['YEARS_BIRTH'], 10)\n",
    "#     # missing_col = (dataframe_train.isna().sum()[dataframe_train.isna().sum() > 0].index).to_list()\n",
    "#     # col_to_group = 'AGE_BIN'\n",
    "#     # col_to_impute = missing_col\n",
    "#     # # vòng for dùng để impute cho nhiều cột\n",
    "#     # for i, col in enumerate(col_to_impute):\n",
    "#     #     # sau khi group kiểu này, grouped chứa 2 phần tử: val_to_group (giá trị được group, ở đây là từng Age_bin) và group_data (dataframe có Age_bin = val_to_group)\n",
    "#     #     grouped = dataframe_train.groupby([col_to_group])[col]\n",
    "#     #     for val_to_group, group_data in grouped:\n",
    "#     #         if dataframe_train[col].dtypes in [int, float]:  # fill biến numeric = mean của từng group_data\n",
    "#     #             mean_value = group_data.mean()\n",
    "#     #             # phải dùng val_to_group[0] là bởi val_to_group được trả về là tuple do hàm cut ban đầu (đừng quan tâm cái này vội)\n",
    "#     #             dataframe_train.loc[(dataframe_train[col_to_group] == val_to_group[0]) & (dataframe_train[col].isna()), col] = mean_value\n",
    "#     #             dataframe_test.loc[(dataframe_test['YEARS_BIRTH'].between(val_to_group[0].left, val_to_group[0].right, inclusive='right')) & (dataframe_test[col].isna()), col] = mean_value\n",
    "#     #         else:  # fill biến câte = mode của từng group_data\n",
    "#     #             mode_value = group_data.mode().iloc[0] if not group_data.mode().empty else np.nan\n",
    "#     #             dataframe_train.loc[(dataframe_train[col_to_group] == val_to_group[0]) & (dataframe_train[col].isna()), col] = mode_value\n",
    "#     #             dataframe_test.loc[(dataframe_test['YEARS_BIRTH'].between(val_to_group[0].left, val_to_group[0].right, inclusive='right')) & (dataframe_test[col].isna()), col] = mode_value\n",
    "#     # dataframe_train.drop(columns='AGE_BIN', inplace=True)\n",
    "#     return dataframe_train, dataframe_test\n",
    "    \n",
    "\n",
    "def fill_missing_app(dataframe_train, dataframe_test):\n",
    "    # 1\n",
    "    mode_gender = dataframe_train['CODE_GENDER'].mode()\n",
    "    dataframe_train['CODE_GENDER'].fillna(mode_gender, inplace=True)\n",
    "    dataframe_test['CODE_GENDER'].fillna(mode_gender, inplace=True)\n",
    "    # 2\n",
    "    mode_year_last_phone = dataframe_train['YEARS_LAST_PHONE_CHANGE'].mode()\n",
    "    dataframe_train['YEARS_LAST_PHONE_CHANGE'].fillna(mode_year_last_phone, inplace=True)\n",
    "    dataframe_test['YEARS_LAST_PHONE_CHANGE'].fillna(mode_year_last_phone, inplace=True)\n",
    "    # 3\n",
    "    mode_day_last_phone = dataframe_train['DAYS_LAST_PHONE_CHANGE'].mode()\n",
    "    dataframe_train['DAYS_LAST_PHONE_CHANGE'].fillna(mode_day_last_phone, inplace=True)\n",
    "    dataframe_test['DAYS_LAST_PHONE_CHANGE'].fillna(mode_day_last_phone, inplace=True)\n",
    "    # 4\n",
    "    mean_fam_mem = dataframe_train['CNT_FAM_MEMBERS'].mean()\n",
    "    dataframe_train['CNT_FAM_MEMBERS'].fillna(mean_fam_mem, inplace=True)\n",
    "    dataframe_test['CNT_FAM_MEMBERS'].fillna(mean_fam_mem, inplace=True)\n",
    "    \n",
    "    # 5, 6, 7, 8, 9\n",
    "    dataframe_train['AGE_BIN'] = pd.cut(dataframe_train['YEARS_BIRTH'], 10)\n",
    "    dataframe_test['AGE_BIN'] = pd.cut(dataframe_test['YEARS_BIRTH'], 10)\n",
    "\n",
    "    col_to_group = 'AGE_BIN'\n",
    "    col_to_impute = ['AMT_ANNUITY', 'AMT_GOODS_PRICE', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']\n",
    "    # vòng for dùng để impute cho nhiều cột\n",
    "    for i, col in enumerate(col_to_impute):\n",
    "        # sau khi group kiểu này, grouped chứa 2 phần tử: val_to_group (giá trị được group, ở đây là từng Age_bin) và group_data (dataframe có Age_bin = val_to_group)\n",
    "        grouped = dataframe_train.groupby([col_to_group])[col]\n",
    "        for val_to_group, group_data in grouped:\n",
    "            if dataframe_train[col].dtypes in [int, float]:  # fill biến numeric = mean của từng group_data\n",
    "                mean_value = group_data.mean()\n",
    "                # phải dùng val_to_group[0] là bởi val_to_group được trả về là tuple do hàm cut ban đầu (đừng quan tâm cái này vội)\n",
    "                dataframe_train.loc[(dataframe_train[col_to_group] == val_to_group[0]) & (dataframe_train[col].isna()), col] = mean_value\n",
    "                dataframe_test.loc[(dataframe_test[col_to_group] == val_to_group[0]) & (dataframe_test[col].isna()), col] = mean_value\n",
    "            else:  # fill biến câte = mode của từng group_data\n",
    "                mode_value = group_data.mode().iloc[0] if not group_data.mode().empty else np.nan\n",
    "                dataframe_train.loc[(dataframe_train[col_to_group] == val_to_group[0]) & (dataframe_train[col].isna()), col] = mode_value\n",
    "                dataframe_test.loc[(dataframe_test[col_to_group] == val_to_group[0]) & (dataframe_test[col].isna()), col] = mode_value\n",
    "    \n",
    "    # 10, 11, 12, 13\n",
    "    col_to_group = 'NAME_INCOME_TYPE'\n",
    "    col_to_impute = ['OWN_CAR_AGE', 'OBS_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE']\n",
    "    # vòng for dùng để impute cho nhiều cột\n",
    "    for i, col in enumerate(col_to_impute):\n",
    "        # sau khi group kiểu này, grouped chứa 2 phần tử: val_to_group (giá trị được group, ở đây là từng Age_bin) và group_data (dataframe có Age_bin = val_to_group)\n",
    "        grouped = dataframe_train.groupby([col_to_group])[col]\n",
    "        for val_to_group, group_data in grouped:\n",
    "            if dataframe_train[col].dtypes in [int, float]:  # fill biến numeric = mean của từng group_data\n",
    "                mean_value = group_data.mean()\n",
    "                # phải dùng val_to_group[0] là bởi val_to_group được trả về là tuple do hàm cut ban đầu (đừng quan tâm cái này vội)\n",
    "                dataframe_train.loc[(dataframe_train[col_to_group] == val_to_group[0]) & (dataframe_train[col].isna()), col] = mean_value\n",
    "                dataframe_test.loc[(dataframe_test[col_to_group] == val_to_group[0]) & (dataframe_test[col].isna()), col] = mean_value\n",
    "            else:  # fill biến câte = mode của từng group_data\n",
    "                mode_value = group_data.mode().iloc[0] if not group_data.mode().empty else np.nan\n",
    "                dataframe_train.loc[(dataframe_train[col_to_group] == val_to_group[0]) & (dataframe_train[col].isna()), col] = mode_value\n",
    "                dataframe_test.loc[(dataframe_test[col_to_group] == val_to_group[0]) & (dataframe_test[col].isna()), col] = mode_value\n",
    "    \n",
    "    # 14\n",
    "    col_to_group = 'REGION_RATING_CLIENT'\n",
    "    col_to_impute = ['DEF_30_CNT_SOCIAL_CIRCLE']\n",
    "    # vòng for dùng để impute cho nhiều cột\n",
    "    for i, col in enumerate(col_to_impute):\n",
    "        # sau khi group kiểu này, grouped chứa 2 phần tử: val_to_group (giá trị được group, ở đây là từng Age_bin) và group_data (dataframe có Age_bin = val_to_group)\n",
    "        grouped = dataframe_train.groupby([col_to_group])[col]\n",
    "        for val_to_group, group_data in grouped:\n",
    "            if dataframe_train[col].dtypes in [int, float]:  # fill biến numeric = mean của từng group_data\n",
    "                mean_value = group_data.mean()\n",
    "                # phải dùng val_to_group[0] là bởi val_to_group được trả về là tuple do hàm cut ban đầu (đừng quan tâm cái này vội)\n",
    "                dataframe_train.loc[(dataframe_train[col_to_group] == val_to_group[0]) & (dataframe_train[col].isna()), col] = mean_value\n",
    "                dataframe_test.loc[(dataframe_test[col_to_group] == val_to_group[0]) & (dataframe_test[col].isna()), col] = mean_value\n",
    "            else:  # fill biến câte = mode của từng group_data\n",
    "                mode_value = group_data.mode().iloc[0] if not group_data.mode().empty else np.nan\n",
    "                dataframe_train.loc[(dataframe_train[col_to_group] == val_to_group[0]) & (dataframe_train[col].isna()), col] = mode_value\n",
    "                dataframe_test.loc[(dataframe_test[col_to_group] == val_to_group[0]) & (dataframe_test[col].isna()), col] = mode_value\n",
    "    \n",
    "    # 15, 16, 17, 18, 19, 20\n",
    "    col_to_group = 'AMT_GOODS_PRICE'\n",
    "    col_to_impute = ['AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR']\n",
    "    # vòng for dùng để impute cho nhiều cột\n",
    "    for i, col in enumerate(col_to_impute):\n",
    "        # sau khi group kiểu này, grouped chứa 2 phần tử: val_to_group (giá trị được group, ở đây là từng Age_bin) và group_data (dataframe có Age_bin = val_to_group)\n",
    "        dataframe_train[f'{col_to_group}_BIN'] = pd.cut(dataframe_train[col_to_group], 10)\n",
    "        grouped = dataframe_train.groupby([f'{col_to_group}_BIN'])[col]\n",
    "        for val_to_group, group_data in grouped:\n",
    "            if dataframe_train[col].dtypes in [int, float]:  # fill biến numeric = mean của từng group_data\n",
    "                mean_value = group_data.mean()\n",
    "                # phải dùng val_to_group[0] là bởi val_to_group được trả về là tuple do hàm cut ban đầu (đừng quan tâm cái này vội)\n",
    "                dataframe_train.loc[(dataframe_train[f'{col_to_group}_BIN'] == val_to_group[0]) & (dataframe_train[col].isna()), col] = mean_value\n",
    "                dataframe_test.loc[(dataframe_test[col_to_group].between(val_to_group[0].left, val_to_group[0].right, inclusive='right')) & (dataframe_test[col].isna()), col] = mean_value\n",
    "            else:  # fill biến câte = mode của từng group_data\n",
    "                mode_value = group_data.mode().iloc[0] if not group_data.mode().empty else np.nan\n",
    "                dataframe_train.loc[(dataframe_train[f'{col_to_group}_BIN'] == val_to_group[0]) & (dataframe_train[col].isna()), col] = mode_value\n",
    "                dataframe_test.loc[(dataframe_test[col_to_group].between(val_to_group[0].left, val_to_group[0].right, inclusive='right')) & (dataframe_test[col].isna()), col] = mode_value\n",
    "    dataframe_train.drop(columns=f'{col_to_group}_BIN', inplace=True)\n",
    "    \n",
    "    # 21\n",
    "    col_to_group = 'AMT_INCOME_TOTAL'\n",
    "    col_to_impute = ['YEARS_EMPLOYED']\n",
    "    # vòng for dùng để impute cho nhiều cột\n",
    "    for i, col in enumerate(col_to_impute):\n",
    "        # sau khi group kiểu này, grouped chứa 2 phần tử: val_to_group (giá trị được group, ở đây là từng Age_bin) và group_data (dataframe có Age_bin = val_to_group)\n",
    "        dataframe_train[f'{col_to_group}_BIN'] = pd.cut(dataframe_train[col_to_group], 10)\n",
    "        grouped = dataframe_train.groupby([f'{col_to_group}_BIN'])[col]\n",
    "        for val_to_group, group_data in grouped:\n",
    "            if dataframe_train[col].dtypes in [int, float]:  # fill biến numeric = mean của từng group_data\n",
    "                mean_value = group_data.mean()\n",
    "                # phải dùng val_to_group[0] là bởi val_to_group được trả về là tuple do hàm cut ban đầu (đừng quan tâm cái này vội)\n",
    "                dataframe_train.loc[(dataframe_train[f'{col_to_group}_BIN'] == val_to_group[0]) & (dataframe_train[col].isna()), col] = mean_value\n",
    "                dataframe_test.loc[(dataframe_test[col_to_group].between(val_to_group[0].left, val_to_group[0].right, inclusive='right')) & (dataframe_test[col].isna()), col] = mean_value\n",
    "            else:  # fill biến câte = mode của từng group_data\n",
    "                mode_value = group_data.mode().iloc[0] if not group_data.mode().empty else np.nan\n",
    "                dataframe_train.loc[(dataframe_train[f'{col_to_group}_BIN'] == val_to_group[0]) & (dataframe_train[col].isna()), col] = mode_value\n",
    "                dataframe_test.loc[(dataframe_test[col_to_group].between(val_to_group[0].left, val_to_group[0].right, inclusive='right')) & (dataframe_test[col].isna()), col] = mode_value\n",
    "    dataframe_train.drop(columns=f'{col_to_group}_BIN', inplace=True)\n",
    "    \n",
    "    dataframe_train.drop(columns='AGE_BIN', inplace=True)\n",
    "    dataframe_test.drop(columns='AGE_BIN', inplace=True)\n",
    "    \n",
    "    return dataframe_train, dataframe_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipping_outliers_app(dataframe_train, dataframe_test):\n",
    "    # for categorical variables which has density < 5%\n",
    "    columns = dataframe_train.select_dtypes('object').columns\n",
    "    for col in columns:\n",
    "        if dataframe_train[col].nunique() > 2:\n",
    "            cond = dataframe_train[col].value_counts(normalize=True)[dataframe_train[col].value_counts(normalize=True) < 0.05].index.to_list()\n",
    "            dataframe_train.loc[dataframe_train[col].isin(cond), col] = 'Others'\n",
    "            dataframe_test.loc[dataframe_test[col].isin(cond), col] = 'Others'\n",
    "    # for numeric variables lied in 1% of distribution\n",
    "    col_to_check = dataframe_train.select_dtypes('number').columns.drop(['SK_ID_CURR', 'TARGET'])\n",
    "    for col in col_to_check:\n",
    "        upper_limit = dataframe_train[col].quantile(.99)\n",
    "        lower_limit = dataframe_train[col].quantile(.01)\n",
    "        dataframe_train.loc[dataframe_train[col] >= upper_limit, col] = upper_limit\n",
    "        dataframe_train.loc[dataframe_train[col] <= lower_limit, col] = lower_limit\n",
    "        dataframe_test.loc[dataframe_test[col] >= upper_limit, col] = upper_limit\n",
    "        dataframe_test.loc[dataframe_test[col] <= lower_limit, col] = lower_limit\n",
    "    return dataframe_train, dataframe_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_app(dataframe):\n",
    "    # flag obs and def on 30, 60 days\n",
    "    dataframe['30_def'] = np.where(dataframe['DEF_30_CNT_SOCIAL_CIRCLE'] == 0, 0, 1)\n",
    "    dataframe['60_def'] = np.where(dataframe['DEF_60_CNT_SOCIAL_CIRCLE'] == 0, 0, 1)\n",
    "    dataframe['30_OBS'] = np.where(dataframe['OBS_30_CNT_SOCIAL_CIRCLE'] == 0, 0, 1)\n",
    "    dataframe['60_OBS'] = np.where(dataframe['OBS_60_CNT_SOCIAL_CIRCLE'] == 0, 0, 1)\n",
    "\n",
    "    # Flag_document features - count and kurtosis\n",
    "    docs = [f for f in dataframe.columns if 'FLAG_DOC' in f]\n",
    "    dataframe['DOCUMENT_COUNT'] = dataframe[docs].sum(axis=1)\n",
    "    dataframe['DOCUMENT_RATIO'] = dataframe['DOCUMENT_COUNT'] / len(docs)\n",
    "    dataframe['NEW_DOC_KURT'] = dataframe[docs].kurtosis(axis=1)\n",
    "    # \"domain knowledge\"\n",
    "    dataframe['CREDIT_TERM'] = dataframe['AMT_ANNUITY'] / dataframe['AMT_CREDIT']\n",
    "    dataframe['credit_downpayment'] = dataframe['AMT_GOODS_PRICE'] - dataframe['AMT_CREDIT']\n",
    "    # Some simple new features (percentages)\n",
    "    dataframe['DAYS_EMPLOYED_PERC'] = dataframe['DAYS_EMPLOYED'] / dataframe['DAYS_BIRTH']\n",
    "    dataframe['INCOME_CREDIT_PERC'] = dataframe['AMT_INCOME_TOTAL'] / dataframe['AMT_CREDIT']\n",
    "    dataframe['INCOME_PER_PERSON'] = dataframe['AMT_INCOME_TOTAL'] / dataframe['CNT_FAM_MEMBERS']\n",
    "    dataframe['ANNUITY_INCOME_PERC'] = dataframe['AMT_ANNUITY'] / dataframe['AMT_INCOME_TOTAL']\n",
    "    dataframe['PAYMENT_RATE'] = dataframe['AMT_ANNUITY'] / dataframe['AMT_CREDIT']\n",
    "    dataframe['CREDIT_TO_GOODS_RATIO'] = dataframe['AMT_CREDIT'] / dataframe['AMT_GOODS_PRICE']\n",
    "    dataframe['INCOME_TO_EMPLOYED_RATIO'] = dataframe['AMT_INCOME_TOTAL'] / dataframe['DAYS_EMPLOYED']\n",
    "    dataframe['INCOME_TO_BIRTH_RATIO'] = dataframe['AMT_INCOME_TOTAL'] / dataframe['DAYS_BIRTH']\n",
    "    dataframe['ID_TO_BIRTH_RATIO'] = dataframe['DAYS_ID_PUBLISH'] / dataframe['DAYS_BIRTH']\n",
    "    dataframe['CAR_TO_BIRTH_RATIO'] = dataframe['OWN_CAR_AGE'] / dataframe['DAYS_BIRTH']\n",
    "    dataframe['CAR_TO_EMPLOYED_RATIO'] = dataframe['OWN_CAR_AGE'] / dataframe['DAYS_EMPLOYED']\n",
    "    dataframe['PHONE_TO_BIRTH_RATIO'] = dataframe['DAYS_LAST_PHONE_CHANGE'] / dataframe['DAYS_BIRTH']\n",
    "    dataframe['APPS_GOODS_INCOME_RATIO'] = dataframe['AMT_GOODS_PRICE'] / dataframe['AMT_INCOME_TOTAL']\n",
    "    dataframe['APPS_CNT_FAM_INCOME_RATIO'] = dataframe['AMT_INCOME_TOTAL'] / dataframe['CNT_FAM_MEMBERS']\n",
    "    dataframe['APPS_INCOME_EMPLOYED_RATIO'] = dataframe['AMT_INCOME_TOTAL'] / dataframe['DAYS_EMPLOYED']\n",
    "\n",
    "\n",
    "    # EXT_SOURCE_X FEATURE\n",
    "    dataframe['APPS_EXT_SOURCE_MEAN'] = dataframe[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "    dataframe['APPS_EXT_SOURCE_STD'] = dataframe[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n",
    "    dataframe['APPS_EXT_SOURCE_STD'] = dataframe['APPS_EXT_SOURCE_STD'].fillna(dataframe['APPS_EXT_SOURCE_STD'].mean())\n",
    "    dataframe['APP_SCORE1_TO_BIRTH_RATIO'] = dataframe['EXT_SOURCE_1'] / (dataframe['DAYS_BIRTH'] / 365.25)\n",
    "    dataframe['APP_SCORE2_TO_BIRTH_RATIO'] = dataframe['EXT_SOURCE_2'] / (dataframe['DAYS_BIRTH'] / 365.25)\n",
    "    dataframe['APP_SCORE3_TO_BIRTH_RATIO'] = dataframe['EXT_SOURCE_3'] / (dataframe['DAYS_BIRTH'] / 365.25)\n",
    "    dataframe['APP_SCORE1_TO_EMPLOY_RATIO'] = dataframe['EXT_SOURCE_1'] / (dataframe['DAYS_EMPLOYED'] / 365.25)\n",
    "    dataframe['APP_EXT_SOURCE_2*EXT_SOURCE_3*DAYS_BIRTH'] = dataframe['EXT_SOURCE_1'] * dataframe['EXT_SOURCE_2'] * dataframe['DAYS_BIRTH']\n",
    "    dataframe['APP_SCORE1_TO_FAM_CNT_RATIO'] = dataframe['EXT_SOURCE_1'] / dataframe['CNT_FAM_MEMBERS']\n",
    "    dataframe['APP_SCORE1_TO_GOODS_RATIO'] = dataframe['EXT_SOURCE_1'] / dataframe['AMT_GOODS_PRICE']\n",
    "    dataframe['APP_SCORE1_TO_CREDIT_RATIO'] = dataframe['EXT_SOURCE_1'] / dataframe['AMT_CREDIT']\n",
    "    dataframe['APP_SCORE1_TO_SCORE2_RATIO'] = dataframe['EXT_SOURCE_1'] / dataframe['EXT_SOURCE_2']\n",
    "    dataframe['APP_SCORE1_TO_SCORE3_RATIO'] = dataframe['EXT_SOURCE_1'] / dataframe['EXT_SOURCE_3']\n",
    "    dataframe['APP_SCORE2_TO_CREDIT_RATIO'] = dataframe['EXT_SOURCE_2'] / dataframe['AMT_CREDIT']\n",
    "    dataframe['APP_SCORE2_TO_REGION_RATING_RATIO'] = dataframe['EXT_SOURCE_2'] / dataframe['REGION_RATING_CLIENT']\n",
    "    dataframe['APP_SCORE2_TO_CITY_RATING_RATIO'] = dataframe['EXT_SOURCE_2'] / dataframe['REGION_RATING_CLIENT_W_CITY']\n",
    "    dataframe['APP_SCORE2_TO_POP_RATIO'] = dataframe['EXT_SOURCE_2'] / dataframe['REGION_POPULATION_RELATIVE']\n",
    "    dataframe['APP_SCORE2_TO_PHONE_CHANGE_RATIO'] = dataframe['EXT_SOURCE_2'] / dataframe['DAYS_LAST_PHONE_CHANGE']\n",
    "    dataframe['APP_EXT_SOURCE_1*EXT_SOURCE_2'] = dataframe['EXT_SOURCE_1'] * dataframe['EXT_SOURCE_2']\n",
    "    dataframe['APP_EXT_SOURCE_1*EXT_SOURCE_3'] = dataframe['EXT_SOURCE_1'] * dataframe['EXT_SOURCE_3']\n",
    "    dataframe['APP_EXT_SOURCE_2*EXT_SOURCE_3'] = dataframe['EXT_SOURCE_2'] * dataframe['EXT_SOURCE_3']\n",
    "    dataframe['APP_EXT_SOURCE_1*DAYS_EMPLOYED'] = dataframe['EXT_SOURCE_1'] * dataframe['DAYS_EMPLOYED']\n",
    "    dataframe['APP_EXT_SOURCE_2*DAYS_EMPLOYED'] = dataframe['EXT_SOURCE_2'] * dataframe['DAYS_EMPLOYED']\n",
    "    dataframe['APP_EXT_SOURCE_3*DAYS_EMPLOYED'] = dataframe['EXT_SOURCE_3'] * dataframe['DAYS_EMPLOYED']\n",
    "    dataframe['EXT_SOURCES_PROD'] = dataframe['EXT_SOURCE_1'] * dataframe['EXT_SOURCE_2'] * dataframe['EXT_SOURCE_3']\n",
    "    dataframe['EXT_SOURCES_WEIGHTED'] = dataframe.EXT_SOURCE_1 * 2 + dataframe.EXT_SOURCE_2 * 1 + dataframe.EXT_SOURCE_3 * 3\n",
    "    for function_name in ['min', 'max', 'mean', 'nanmedian', 'var']:\n",
    "        feature_name = 'EXT_SOURCES_{}'.format(function_name.upper())\n",
    "        dataframe[feature_name] = eval('np.{}'.format(function_name))(\n",
    "            dataframe[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']], axis=1)\n",
    "\n",
    "\n",
    "    # other feature from better than 0.8\n",
    "    dataframe['CREDIT_TO_GOODS_RATIO_2'] = dataframe['AMT_CREDIT'] / dataframe['AMT_GOODS_PRICE']\n",
    "    dataframe['APP_AMT_INCOME_TOTAL_12_AMT_ANNUITY_ratio'] = dataframe['AMT_INCOME_TOTAL'] / 12. - dataframe['AMT_ANNUITY']\n",
    "    dataframe['APP_INCOME_TO_EMPLOYED_RATIO'] = dataframe['AMT_INCOME_TOTAL'] / dataframe['DAYS_EMPLOYED']\n",
    "    dataframe['APP_DAYS_LAST_PHONE_CHANGE_DAYS_EMPLOYED_ratio'] = dataframe['DAYS_LAST_PHONE_CHANGE'] / dataframe['DAYS_EMPLOYED']\n",
    "    dataframe['APP_DAYS_EMPLOYED_DAYS_BIRTH_diff'] = dataframe['DAYS_EMPLOYED'] - dataframe['DAYS_BIRTH']\n",
    "    \n",
    "    def get_age_label(days_birth):\n",
    "        \"\"\" Return the age group label (int). \"\"\"\n",
    "        age_years = -days_birth / 365\n",
    "        if age_years < 27: return 1\n",
    "        elif age_years < 40: return 2\n",
    "        elif age_years < 50: return 3\n",
    "        elif age_years < 65: return 4\n",
    "        elif age_years < 99: return 5\n",
    "        else: return 0\n",
    "    dataframe['AGE_RANGE'] = dataframe['DAYS_BIRTH'].apply(lambda x: get_age_label(x))\n",
    "    dataframe['UNDER_40'] = np.where(dataframe['YEARS_BIRTH'] <= 40, 1, 0)\n",
    "    dataframe['30_to_40'] = np.where(dataframe['YEARS_BIRTH'].between(30, 40), 1, 0)\n",
    "    dataframe['FIRED_3YEARS'] = np.where(dataframe['YEARS_EMPLOYED'] <= 3, 1, 0)\n",
    "    dataframe['FIRED_10YEARS'] = np.where(dataframe['YEARS_EMPLOYED'] <= 10, 1, 0)\n",
    "    dataframe['LOYAL_EMPLOYEE'] = np.where(dataframe['YEARS_EMPLOYED'] > 10, 1, 0)\n",
    "    dataframe['credit_to_annuity_ratio'] = dataframe['AMT_CREDIT'] / dataframe['AMT_ANNUITY']\n",
    "    dataframe['payment_rate'] = dataframe['AMT_ANNUITY'] / dataframe['AMT_CREDIT']\n",
    "    dataframe['HOUSETYPE_MODE_CHECK'] = np.where(dataframe['HOUSETYPE_MODE'] == 'block of flats', 1, 0)\n",
    "    dataframe['NAME_TYPE_SUITE_CHECK'] = np.where(dataframe['NAME_TYPE_SUITE'] == 'Unaccompanied', 1, 0)\n",
    "    dataframe['NAME_INCOME_TYPE_CHECK'] = np.where(dataframe['NAME_INCOME_TYPE'] == 'Working', 1, 0)\n",
    "    dataframe['NAME_EDUCATION_TYPE_CHECK'] = np.where(dataframe['NAME_EDUCATION_TYPE'] == 'Secondary / secondary special', 1, 0)\n",
    "    dataframe['NAME_FAMILY_STATUS_CHECK'] = np.where(dataframe['NAME_FAMILY_STATUS'] == 'Married', 1, 0)\n",
    "    dataframe['NAME_HOUSING_TYPE_CHECK'] = np.where(dataframe['NAME_HOUSING_TYPE'] == 'House / apartment', 1, 0)\n",
    "    dataframe['OCCUPATION_TYPE_CHECK'] = np.where(dataframe['OCCUPATION_TYPE'] == 'Married', 1, 0)\n",
    "    dataframe['NAME_FAMILY_STATUS_CHECK'] = np.where(dataframe['NAME_FAMILY_STATUS'] == 'Laborers', 1, 0)\n",
    "    dataframe['FONDKAPREMONT_MODE_CHECK'] = np.where(dataframe['FONDKAPREMONT_MODE'].isin(['reg oper account']), 1, 0)\n",
    "    dataframe['WALLSMATERIAL_MODE_CHECK'] = np.where(dataframe['WALLSMATERIAL_MODE'].isin(['Panel']), 1, 0)\n",
    "    dataframe['EMERGENCYSTATE_MODE_CHECK'] = np.where(dataframe['EMERGENCYSTATE_MODE'].isin(['No']), 1, 0)\n",
    "    dataframe['CNT_CHILDREN_CHECK'] = np.where(dataframe['CNT_CHILDREN'] == 0, 1, 0)\n",
    "    dataframe['CNT_FAM_MEMBERS_CHECK'] = np.where(dataframe['CNT_FAM_MEMBERS'] <= 2, 1, 0)\n",
    "    dataframe['CNT_FAM_MEMBERS_CHECK'] = np.where(dataframe['CNT_FAM_MEMBERS'] <= 2, 1, 0)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_feature(dataframe, target='TARGET', exclude_list=['SK_ID_CURR', 'TARGET'],\n",
    "                   empty=0.5, iv=0.02, corr=0.8):\n",
    "    train_selected, drop_lst= toad.selection.select(frame = dataframe,\n",
    "                                                    target=dataframe[target], \n",
    "                                                    empty = empty, \n",
    "                                                    iv = iv, corr = corr, \n",
    "                                                    return_drop=True, \n",
    "                                                    exclude=exclude_list)\n",
    "    # frame: dataset\n",
    "    # target: target column\n",
    "    # empty: remove feature with > 60% empty\n",
    "    # iv: retain feature with iv > 0.02\n",
    "    # corr: retain feature with corr < 0.9\n",
    "    # return_drop = True: return the column dropped\n",
    "    # exclude: a list of column that should not be assess and drop (id, label)\n",
    "    print(\"keep:\",train_selected.shape[1],\n",
    "        \"\\ndrop iv:\",len(drop_lst['iv']),\n",
    "        \"\\ndrop empty:\",len(drop_lst['empty']),\n",
    "        \"\\ndrop corr:\",len(drop_lst['corr']))\n",
    "    return train_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def application():\n",
    "    # import data\n",
    "    train = pd.read_csv('data/application_train.csv')\n",
    "    test = pd.read_csv('data/application_test.csv')\n",
    "\n",
    "    # fill missing\n",
    "    \n",
    "    train = prep_fill_missing_app(train)\n",
    "    test = prep_fill_missing_app(test)\n",
    "    \n",
    "    train, test = fill_missing_app(train, test)\n",
    "    # clipping outliers\n",
    "    \n",
    "    train, test = clipping_outliers_app(train, test)\n",
    "    # Combine train and test back after cleaning\n",
    "    df = pd.concat([train, test], axis=0)\n",
    "    df = create_feature_app(df)\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    # Categorical features with Binary encode (0 or 1; two categories)\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    \n",
    "    # Categorical features with One-Hot encode\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category=True)\n",
    "    df.drop(columns='Unnamed: 0', inplace=True)\n",
    "    train = df[df['TARGET'].notna()]\n",
    "    test = df[df['TARGET'].isna()]\n",
    "    train = select_feature(train)\n",
    "    df = pd.concat([train, test[train.columns]], axis=0)\n",
    "    print('\"Application_Train_Test\" final shape:', df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Bureau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_month_bal_mean_bureau(x):\n",
    "    if abs(x) >= 30:\n",
    "        x = abs(x)//30\n",
    "        mean_value = np.mean(np.floor(np.arange(0, x)/ 12))\n",
    "        return mean_value\n",
    "    return 0\n",
    "\n",
    "def lazy_fill_missing_bureau(dataframe_train, dataframe_test):\n",
    "    col_num = dataframe_train.select_dtypes('number').columns\n",
    "    for col in col_num:\n",
    "        median_value = dataframe_train[col].median()\n",
    "        dataframe_train[col] = dataframe_train[col].fillna(median_value)\n",
    "        dataframe_test[col] = dataframe_test[col].fillna(median_value)\n",
    "    col_object = dataframe_train.select_dtypes('object').columns\n",
    "    for col in col_object:\n",
    "        mode_value = dataframe_train[col].mode()[0]\n",
    "        dataframe_train[col] = dataframe_train[col].fillna(mode_value)\n",
    "        dataframe_test[col] = dataframe_test[col].fillna(mode_value)\n",
    "    return dataframe_train, dataframe_test\n",
    "\n",
    "def fill_missing_bureau(df_train, df_test, aggregated_bureau_balance):\n",
    "    common_cols = df_train.columns.intersection(aggregated_bureau_balance.columns)\n",
    "    common_cols = common_cols.drop(['MONTHS_BALANCE_MEAN', 'MONTHS_BALANCE_MAX'])\n",
    "\n",
    "    df_train['MONTHS_BALANCE_MEAN'] = df_train['MONTHS_BALANCE_MEAN'].fillna(df_train['DAYS_CREDIT'].apply(lambda x: fill_month_bal_mean_bureau(x) if pd.notna(x) else None))\n",
    "    df_test['MONTHS_BALANCE_MEAN'] = df_test['MONTHS_BALANCE_MEAN'].fillna(df_test['DAYS_CREDIT'].apply(lambda x: fill_month_bal_mean_bureau(x) if pd.notna(x) else None))\n",
    "    \n",
    "    df_train['MONTHS_BALANCE_MAX'] = df_train['MONTHS_BALANCE_MAX'].fillna(df_train['DAYS_CREDIT'].apply(lambda x: abs(x)//365 if pd.notna(x) else None))\n",
    "    df_test['MONTHS_BALANCE_MAX'] = df_test['MONTHS_BALANCE_MAX'].fillna(df_test['DAYS_CREDIT'].apply(lambda x: abs(x)//365 if pd.notna(x) else None))\n",
    "\n",
    "    #coi nhu cac khoan vay o trang thai unknown la X => de bang 1\n",
    "    group = df_train[(df_train['STATUS_MEAN']==1)&(df_train['STATUS_MAX']==1)&(df_train['STATUS_FIRST']==1)]\n",
    "\n",
    "    numeric_cols = group.select_dtypes(include=['number']).columns\n",
    "    mean_values = group.groupby('MONTHS_BALANCE_MAX')[numeric_cols].mean()\n",
    "    \n",
    "    for col in common_cols:\n",
    "        df_train[col] = df_train.apply(lambda row: mean_values.loc[row['MONTHS_BALANCE_MAX'], col] if pd.isna(row[col]) else row[col],\n",
    "            axis=1)\n",
    "        df_test[col] = df_test.apply(lambda row: mean_values.loc[row['MONTHS_BALANCE_MAX'], col] if pd.isna(row[col]) else row[col],\n",
    "            axis=1)\n",
    "        \n",
    "    # fill nhung khoan vay da close coi nhu la tra dung han\n",
    "    df_train['DAYS_CREDIT_ENDDATE'] = df_train['DAYS_CREDIT_ENDDATE'].fillna(df_train['DAYS_ENDDATE_FACT'])\n",
    "    df_test['DAYS_CREDIT_ENDDATE'] = df_test['DAYS_CREDIT_ENDDATE'].fillna(df_test['DAYS_ENDDATE_FACT'])\n",
    "\n",
    "    #fill nhung khoan vay active \n",
    "    df_train['DAYS_CREDIT_ENDDATE'] = df_train['DAYS_CREDIT_ENDDATE'].fillna(np.round(df_train[df_train['CREDIT_ACTIVE']=='Active']['DAYS_CREDIT_ENDDATE'].mean()))\n",
    "    df_test['DAYS_CREDIT_ENDDATE'] = df_test['DAYS_CREDIT_ENDDATE'].fillna(np.round(df_train[df_train['CREDIT_ACTIVE']=='Active']['DAYS_CREDIT_ENDDATE'].mean()))\n",
    "\n",
    "    # train_df['AMT_ANNUITY']\n",
    "    # train_df['AMT_CREDIT_MAX_OVERDUE']\n",
    "    # train_df['AMT_CREDIT_SUM_DEBT']\n",
    "    # train_df['AMT_CREDIT_SUM_LIMIT']\n",
    "    # train_df['AMT_CREDIT_SUM']\n",
    "    df_train.drop(columns=['AMT_ANNUITY', 'AMT_CREDIT_MAX_OVERDUE'], inplace=True)\n",
    "    df_test.drop(columns=['AMT_ANNUITY', 'AMT_CREDIT_MAX_OVERDUE'], inplace=True)\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    col_to_impute = ['AMT_CREDIT_SUM_DEBT', 'AMT_CREDIT_SUM_LIMIT', 'AMT_CREDIT_SUM']\n",
    "    df_train[col_to_impute] = imputer.fit_transform(df_train[col_to_impute])\n",
    "    df_test[col_to_impute] = imputer.transform(df_test[col_to_impute])\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bureau_balance(df):\n",
    "\n",
    "    #label encode STATUS\n",
    "    dict_for_status = { 'C': 12, 'X': 11, '0': 10, '1': 5, '2': 4, '3': 3, '4': 2, '5': 1}\n",
    "    df['STATUS'] = df['STATUS'].map(dict_for_status)\n",
    "\n",
    "    #converting months to positive\n",
    "    #weighing the status with the months_balance\n",
    "    df['MONTHS_BALANCE'] = np.abs(df['MONTHS_BALANCE'])\n",
    "    df['WEIGHTED_STATUS'] = df.STATUS / (df.MONTHS_BALANCE + 1)\n",
    "\n",
    "    #sorting the bureau_balance in ascending order of month and by the bureau SK_ID\n",
    "    df = df.sort_values(by=['SK_ID_BUREAU', 'MONTHS_BALANCE'], ascending=[0, 0])\n",
    "    #we will do exponential weighted average on the encoded status\n",
    "    #this is because if a person had a bad status 2 years ago, it should be given less weightage today\n",
    "    # we keep the latent variable alpha = 0.8 \n",
    "    #doing this for both weighted status and the status itself\n",
    "    df['EXP_WEIGHTED_STATUS'] = df.groupby('SK_ID_BUREAU')['WEIGHTED_STATUS'].transform(lambda x: x.ewm(alpha = 0.8).mean())\n",
    "    df['EXP_ENCODED_STATUS'] = df.groupby('SK_ID_BUREAU')['STATUS'].transform(lambda x: x.ewm(alpha = 0.8).mean())    \n",
    "\n",
    "    df['MONTHS_BALANCE'] = df['MONTHS_BALANCE'] // 12\n",
    "\n",
    "    #defining our aggregations\n",
    "    aggregations_basic = {\n",
    "        'MONTHS_BALANCE' : ['mean','max'],\n",
    "        'STATUS' : ['mean','max','first'],\n",
    "        'WEIGHTED_STATUS' : ['mean','sum','first'],\n",
    "        'EXP_ENCODED_STATUS' : ['last'],\n",
    "        'EXP_WEIGHTED_STATUS' : ['last']}\n",
    "\n",
    "    #we will be finding aggregates for each year too\n",
    "    aggregations_for_year = {\n",
    "        'STATUS' : ['mean','max','last','first'],\n",
    "        'WEIGHTED_STATUS' : ['mean','max', 'first','last'],\n",
    "        'EXP_WEIGHTED_STATUS' : ['last'],\n",
    "        'EXP_ENCODED_STATUS' : ['last'] }\n",
    "\n",
    "    #aggregating over whole dataset first\n",
    "    aggregated_df = df.groupby(['SK_ID_BUREAU']).agg(aggregations_basic)\n",
    "    aggregated_df.columns = ['_'.join(ele).upper() for ele in aggregated_df.columns]\n",
    "\n",
    "    #aggregating some of the features separately for latest 2 years\n",
    "    aggregated_df_years = pd.DataFrame()\n",
    "    for year in range(2):\n",
    "        year_group = df[df['MONTHS_BALANCE'] == year].groupby('SK_ID_BUREAU').agg(aggregations_for_year)\n",
    "        year_group.columns = ['_'.join(ele).upper() + '_YEAR_' + str(year) for ele in year_group.columns]\n",
    "\n",
    "        if year == 0:\n",
    "            aggregated_df_years = year_group\n",
    "        else:\n",
    "            aggregated_df_years = aggregated_df_years.merge(year_group, on = 'SK_ID_BUREAU', how = 'outer')\n",
    "\n",
    "    #aggregating for rest of the years\n",
    "    aggregated_df_rest_years = df[df.MONTHS_BALANCE > year].groupby(['SK_ID_BUREAU']).agg(aggregations_for_year)\n",
    "    aggregated_df_rest_years.columns = ['_'.join(ele).upper() + '_YEAR_REST' for ele in aggregated_df_rest_years.columns]\n",
    "\n",
    "    #merging with rest of the years\n",
    "    aggregated_df_years = aggregated_df_years.merge(aggregated_df_rest_years, on = 'SK_ID_BUREAU', how = 'outer')\n",
    "    aggregated_df = aggregated_df.merge(aggregated_df_years, on = 'SK_ID_BUREAU', how = 'inner')\n",
    "\n",
    "    #filling the missing values obtained after aggregations with 0\n",
    "    aggregated_df.fillna(0, inplace = True)\n",
    "\n",
    "    return aggregated_df\n",
    "\n",
    "def create_bureau(df):\n",
    "    #create features based on domain\n",
    "    df['CREDIT_DURATION'] = np.abs(df['DAYS_CREDIT'] - df['DAYS_CREDIT_ENDDATE'])\n",
    "    df['FLAG_OVERDUE_RECENT'] = [0 if ele == 0 else 1 for ele in df['CREDIT_DAY_OVERDUE']]\n",
    "    # df['MAX_AMT_OVERDUE_DURATION_RATIO'] = df['AMT_CREDIT_MAX_OVERDUE'] / (df['CREDIT_DURATION'] + 0.00001)\n",
    "    df['CURRENT_AMT_OVERDUE_DURATION_RATIO'] = df['AMT_CREDIT_SUM_OVERDUE'] / (df['CREDIT_DURATION'] + 0.00001)\n",
    "    df['AMT_OVERDUE_DURATION_LEFT_RATIO'] = df['AMT_CREDIT_SUM_OVERDUE'] / (df['DAYS_CREDIT_ENDDATE'] + 0.00001)\n",
    "    # df['CNT_PROLONGED_MAX_OVERDUE_MUL'] = df['CNT_CREDIT_PROLONG'] * df['AMT_CREDIT_MAX_OVERDUE']\n",
    "    df['CNT_PROLONGED_DURATION_RATIO'] = df['CNT_CREDIT_PROLONG'] / (df['CREDIT_DURATION'] + 0.00001)\n",
    "    df['CURRENT_DEBT_TO_CREDIT_RATIO'] = df['AMT_CREDIT_SUM_DEBT'] / (df['AMT_CREDIT_SUM'] + 0.00001)\n",
    "    df['CURRENT_CREDIT_DEBT_DIFF'] = df['AMT_CREDIT_SUM'] - df['AMT_CREDIT_SUM_DEBT']\n",
    "    # df['AMT_ANNUITY_CREDIT_RATIO'] = df['AMT_ANNUITY'] / (df['AMT_CREDIT_SUM'] + 0.00001)\n",
    "    df['CREDIT_ENDDATE_UPDATE_DIFF'] = np.abs(df['DAYS_CREDIT_UPDATE'] - df['DAYS_CREDIT_ENDDATE'])\n",
    "\n",
    "    #now we will be aggregating the bureau_merged df with respect to 'SK_ID_CURR' so as to merge it with application_train later    \n",
    "    #firstly we will aggregate the columns based on the category of CREDIT_ACTIVE\n",
    "    aggregations_CREDIT_ACTIVE = {\n",
    "                    'DAYS_CREDIT' : ['mean','min','max','last'],\n",
    "                    'CREDIT_DAY_OVERDUE' : ['mean','max'],\n",
    "                    'DAYS_CREDIT_ENDDATE' : ['mean','max'],\n",
    "                    'DAYS_ENDDATE_FACT' : ['mean','min'],\n",
    "                    # 'AMT_CREDIT_MAX_OVERDUE': ['max','sum'],\n",
    "                    'CNT_CREDIT_PROLONG': ['max','sum'],\n",
    "                    'AMT_CREDIT_SUM' : ['sum','max'],\n",
    "                    'AMT_CREDIT_SUM_DEBT': ['sum'],\n",
    "                    'AMT_CREDIT_SUM_LIMIT': ['max','sum'],\n",
    "                    'AMT_CREDIT_SUM_OVERDUE': ['max','sum'],\n",
    "                    'DAYS_CREDIT_UPDATE' : ['mean','min'],\n",
    "                    # 'AMT_ANNUITY' : ['mean','sum','max'],\n",
    "                    'CREDIT_DURATION' : ['max','mean'],\n",
    "                    'FLAG_OVERDUE_RECENT': ['sum'],\n",
    "                    # 'MAX_AMT_OVERDUE_DURATION_RATIO' : ['max','sum'],\n",
    "                    'CURRENT_AMT_OVERDUE_DURATION_RATIO' : ['max','sum'],\n",
    "                    'AMT_OVERDUE_DURATION_LEFT_RATIO' : ['max', 'mean'],\n",
    "                    # 'CNT_PROLONGED_MAX_OVERDUE_MUL' : ['mean','max'],\n",
    "                    'CNT_PROLONGED_DURATION_RATIO' : ['mean', 'max'],\n",
    "                    'CURRENT_DEBT_TO_CREDIT_RATIO' : ['mean', 'min'],\n",
    "                    'CURRENT_CREDIT_DEBT_DIFF' : ['mean','min'],\n",
    "                    # 'AMT_ANNUITY_CREDIT_RATIO' : ['mean','max','min'],\n",
    "                    'CREDIT_ENDDATE_UPDATE_DIFF' : ['max','min'],\n",
    "                    'STATUS_MEAN' : ['mean', 'max'],\n",
    "                    'WEIGHTED_STATUS_MEAN' : ['mean', 'max']\n",
    "                        }\n",
    "\n",
    "    #we saw from EDA that the two most common type of CREDIT ACTIVE were 'Closed' and 'Active'.\n",
    "    #So we will aggregate them two separately and the remaining categories separately.\n",
    "    categories_to_aggregate_on = ['Closed','Active']\n",
    "    df_aggregated_credit = pd.DataFrame()\n",
    "    for i, status in enumerate(categories_to_aggregate_on):\n",
    "        group = df[df['CREDIT_ACTIVE'] == status].groupby('SK_ID_CURR').agg(aggregations_CREDIT_ACTIVE)\n",
    "        group.columns = ['_'.join(ele).upper() + '_CREDITACTIVE_' + status.upper() for ele in group.columns]\n",
    "\n",
    "        if i==0:\n",
    "            df_aggregated_credit = group\n",
    "        else:\n",
    "            df_aggregated_credit = df_aggregated_credit.merge(group, on = 'SK_ID_CURR', how = 'outer')\n",
    "    \n",
    "    #aggregating for remaining categories\n",
    "    df_aggregated_credit_rest = df[(df['CREDIT_ACTIVE'] != 'Active') & \n",
    "                                                            (df['CREDIT_ACTIVE'] != 'Closed')].groupby('SK_ID_CURR').agg(aggregations_CREDIT_ACTIVE)\n",
    "    df_aggregated_credit_rest.columns = ['_'.join(ele).upper() + 'CREDIT_ACTIVE_REST' for ele in df_aggregated_credit_rest.columns]\n",
    "\n",
    "    #merging with other categories\n",
    "    df_aggregated_credit = df_aggregated_credit.merge(df_aggregated_credit_rest, on = 'SK_ID_CURR', how = 'outer')\n",
    "\n",
    "    #Encoding the categorical columns in one-hot form\n",
    "    currency_ohe = pd.get_dummies(df['CREDIT_CURRENCY'], prefix = 'CURRENCY')\n",
    "    credit_active_ohe = pd.get_dummies(df['CREDIT_ACTIVE'], prefix = 'CREDIT_ACTIVE')\n",
    "    credit_type_ohe = pd.get_dummies(df['CREDIT_TYPE'], prefix = 'CREDIT_TYPE')\n",
    "\n",
    "    #merging the one-hot encoded columns\n",
    "    df = pd.concat([df.drop(['CREDIT_CURRENCY','CREDIT_ACTIVE','CREDIT_TYPE'], axis = 1), \n",
    "                                currency_ohe, credit_active_ohe, credit_type_ohe], axis = 1)\n",
    "\n",
    "    #aggregating the bureau_merged over all the columns\n",
    "    df_aggregated = df.drop('SK_ID_BUREAU', axis = 1).groupby('SK_ID_CURR').agg('mean')\n",
    "    df_aggregated.columns = [ele if 'TARGET' in ele else ele + '_MEAN_OVERALL' for ele in df_aggregated.columns]\n",
    "    #merging it with aggregates over categories\n",
    "    df_aggregated = df_aggregated.merge(df_aggregated_credit, on = 'SK_ID_CURR', how = 'outer')\n",
    "\n",
    "    return df_aggregated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Main Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bureau_and_balance():\n",
    "    bureau_balance = pd.read_csv('data/bureau_balance.csv')\n",
    "    bureau = pd.read_csv('data/bureau.csv')\n",
    "    target = pd.read_csv('data/target.csv')\n",
    "    \n",
    "    #merge bureau_balance with bureau to get SK_ID_CURR\n",
    "    bb_target = bureau_balance.merge(bureau[['SK_ID_BUREAU', 'SK_ID_CURR']], on='SK_ID_BUREAU', how='left')\n",
    "    #merge bureau_balance with target\n",
    "    bb_target = bb_target.merge(target, on='SK_ID_CURR', how='left')\n",
    "    bb_target = bb_target.drop('SK_ID_CURR', axis=1)\n",
    "\n",
    "    #split bb into train and test\n",
    "    bb_train = bb_target[bb_target['TARGET'].notnull()]\n",
    "    bb_test = bb_target[bb_target['TARGET'].isnull()]\n",
    "    bb_train = bb_train.drop('TARGET', axis=1)\n",
    "    bb_test = bb_test.drop('TARGET', axis=1)\n",
    "\n",
    "    #create fetures for bb\n",
    "    bb_train = create_bureau_balance(bb_train)\n",
    "    bb_test = create_bureau_balance(bb_test)\n",
    "\n",
    "    #merge bureau with target\n",
    "    bureau_target = bureau.merge(target, on = 'SK_ID_CURR', how = 'left')\n",
    "    bureau_train = bureau_target[bureau_target['TARGET'].notnull()]\n",
    "    bureau_test = bureau_target[bureau_target['TARGET'].isnull()]\n",
    "    \n",
    "    \n",
    "    #merge train, test with bureau\n",
    "    train = bureau_train.merge(bb_train, on = 'SK_ID_BUREAU', how = 'left')\n",
    "    test = bureau_test.merge(bb_test, on = 'SK_ID_BUREAU', how = 'left')\n",
    "\n",
    "    # cleaning\n",
    "    # train, test = fill_missing_bureau(train, test, bb_train)\n",
    "    train, test = lazy_fill_missing_bureau(train, test)\n",
    "    train, test = clipping_outliers_app(train, test)\n",
    "    # FE \n",
    "    train = create_bureau(train)\n",
    "    test = create_bureau(test)\n",
    "    # return train, test\n",
    "    #merge train and test\n",
    "    df = pd.concat([train, test], axis = 0)\n",
    "\n",
    "    # # clean some invalid data after FE\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    train = df[df['TARGET'].notna()]\n",
    "    test = df[df['TARGET'].isna()]\n",
    "    train = select_feature(train)\n",
    "    df = pd.concat([train, test[train.columns]], axis=0)\n",
    "    if 'TARGET' in df.columns: df.drop(columns='TARGET', inplace=True)\n",
    "    # sr_na = df.isna().sum()\n",
    "    # sr_na = sr_na / len(df) * 100\n",
    "    # col_to_drop = ['TARGET'] + sr_na[sr_na > 50].index.to_list()\n",
    "    # df.drop(columns=col_to_drop, inplace=True)\n",
    "\n",
    "    # train = df[df['TARGET'].notna()]\n",
    "    # test = df[df['TARGET'].isna()]\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEMPORARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bureau_and_balance_temp():\n",
    "    bureau = pd.read_csv(r'data/bureau.csv')\n",
    "    bb = pd.read_csv(r'data/bureau_balance.csv')\n",
    "\n",
    "    # Credit duration and credit/account end date difference\n",
    "    bureau['CREDIT_DURATION'] = -bureau['DAYS_CREDIT'] + bureau['DAYS_CREDIT_ENDDATE']\n",
    "    bureau['ENDDATE_DIF'] = bureau['DAYS_CREDIT_ENDDATE'] - bureau['DAYS_ENDDATE_FACT']\n",
    "    \n",
    "    # Credit to debt ratio and difference\n",
    "    bureau['DEBT_PERCENTAGE'] = bureau['AMT_CREDIT_SUM'] / bureau['AMT_CREDIT_SUM_DEBT']\n",
    "    bureau['DEBT_CREDIT_DIFF'] = bureau['AMT_CREDIT_SUM'] - bureau['AMT_CREDIT_SUM_DEBT']\n",
    "    bureau['CREDIT_TO_ANNUITY_RATIO'] = bureau['AMT_CREDIT_SUM'] / bureau['AMT_ANNUITY']\n",
    "    bureau['BUREAU_CREDIT_FACT_DIFF'] = bureau['DAYS_CREDIT'] - bureau['DAYS_ENDDATE_FACT']\n",
    "    bureau['BUREAU_CREDIT_ENDDATE_DIFF'] = bureau['DAYS_CREDIT'] - bureau['DAYS_CREDIT_ENDDATE']\n",
    "    bureau['BUREAU_CREDIT_DEBT_RATIO'] = bureau['AMT_CREDIT_SUM_DEBT'] / bureau['AMT_CREDIT_SUM']\n",
    "\n",
    "    # CREDIT_DAY_OVERDUE :\n",
    "    bureau['BUREAU_IS_DPD'] = bureau['CREDIT_DAY_OVERDUE'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    bureau['BUREAU_IS_DPD_OVER120'] = bureau['CREDIT_DAY_OVERDUE'].apply(lambda x: 1 if x > 120 else 0)\n",
    "\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "\n",
    "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size', 'mean']}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "\n",
    "    #Status of Credit Bureau loan during the month\n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "\n",
    "    # Bureau and bureau_balance numeric features\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean', 'min'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean', 'max'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean', 'max', 'sum'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean', 'sum'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum'],\n",
    "        'SK_ID_BUREAU': ['count'],\n",
    "        'DAYS_ENDDATE_FACT': ['min', 'max', 'mean'],\n",
    "        'ENDDATE_DIF': ['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_FACT_DIFF': ['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_ENDDATE_DIFF': ['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_DEBT_RATIO': ['min', 'max', 'mean'],\n",
    "        'DEBT_CREDIT_DIFF': ['min', 'max', 'mean'],\n",
    "        'BUREAU_IS_DPD': ['mean', 'sum'],\n",
    "        'BUREAU_IS_DPD_OVER120': ['mean', 'sum']\n",
    "        }\n",
    "\n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "\n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    # Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "    bureau_agg.reset_index(inplace=True)\n",
    "    print('\"Bureau/Bureau Balance\" final shape:', bureau_agg.shape)\n",
    "    \n",
    "    return bureau_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = bureau_bb()\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Credit Card Balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #MinhHuong - fixed, add ewm\n",
    "# def lazy_fill_missing_cc(dataframe_train, dataframe_test):\n",
    "#     col_num = dataframe_train.select_dtypes('number').columns.drop('TARGET')\n",
    "#     for col in col_num:\n",
    "#         median_value = dataframe_train[col].median()\n",
    "#         dataframe_train[col] = dataframe_train[col].fillna(median_value)\n",
    "#         dataframe_test[col] = dataframe_test[col].fillna(median_value)\n",
    "\n",
    "#     col_object = dataframe_train.select_dtypes('object').columns\n",
    "#     for col in col_object:\n",
    "#         mode_value = dataframe_train[col].mode()[0]\n",
    "#         dataframe_train[col] = dataframe_train[col].fillna(mode_value)\n",
    "#         dataframe_test[col] = dataframe_test[col].fillna(mode_value)\n",
    "#     return dataframe_train, dataframe_test\n",
    "\n",
    "def missing_values(df_train, df_test):\n",
    "    drawings_cols = ['AMT_DRAWINGS_ATM_CURRENT', 'AMT_DRAWINGS_POS_CURRENT',\n",
    "                'AMT_DRAWINGS_OTHER_CURRENT', 'CNT_DRAWINGS_ATM_CURRENT',\n",
    "                'CNT_DRAWINGS_OTHER_CURRENT', 'CNT_DRAWINGS_POS_CURRENT']\n",
    "\n",
    "    # fill 0 for AMT_BALANCE = 0 -> no transactions are made\n",
    "    df_train.loc[df_train['AMT_BALANCE'] == 0, drawings_cols].fillna(0)\n",
    "    df_test.loc[df_test['AMT_BALANCE'] == 0, drawings_cols].fillna(0)\n",
    "\n",
    "    # Fill remaining missing values in drawings_cols with 0 if both AMT_DRAWINGS_CURRENT and CNT_DRAWINGS_CURRENT are 0\n",
    "    df_train.loc[(df_train['AMT_DRAWINGS_CURRENT'] == 0) & (df_train['CNT_DRAWINGS_CURRENT'] == 0), drawings_cols].fillna(0)\n",
    "    df_test.loc[(df_test['AMT_DRAWINGS_CURRENT'] == 0) & (df_test['CNT_DRAWINGS_CURRENT'] == 0), drawings_cols].fillna(0)\n",
    "\n",
    "    # Fill AMT_PAYMENT_CURRENT\n",
    "    mean_payment = df_train['AMT_PAYMENT_CURRENT'].mean()\n",
    "    df_train['AMT_PAYMENT_CURRENT'].fillna(mean_payment, inplace=True)\n",
    "    df_test['AMT_PAYMENT_CURRENT'].fillna(mean_payment, inplace=True)\n",
    "\n",
    "    #fill AMT_INST_MIN_REGULARITY\n",
    "    payment_ratio = df_train['AMT_INST_MIN_REGULARITY'].dropna() / df_train['AMT_CREDIT_LIMIT_ACTUAL']\n",
    "    median_ratio = payment_ratio.median()\n",
    "\n",
    "    missing_mask_train = df_train['AMT_INST_MIN_REGULARITY'].isna()\n",
    "    missing_mask_test = df_test['AMT_INST_MIN_REGULARITY'].isna()\n",
    "    df_train.loc[missing_mask_train, 'AMT_INST_MIN_REGULARITY'] = df_train.loc[missing_mask_train, 'AMT_CREDIT_LIMIT_ACTUAL'] * median_ratio\n",
    "    df_test.loc[missing_mask_test, 'AMT_INST_MIN_REGULARITY'] = df_test.loc[missing_mask_test, 'AMT_CREDIT_LIMIT_ACTUAL'] * median_ratio\n",
    "\n",
    "    #fill CNT_INSTALMENT_MATURE_CUM\n",
    "    mean_cnt = df_train['CNT_INSTALMENT_MATURE_CUM'].mean().round()\n",
    "    df_train['CNT_INSTALMENT_MATURE_CUM'].fillna(mean_cnt, inplace=True)\n",
    "    df_test['CNT_INSTALMENT_MATURE_CUM'].fillna(mean_cnt, inplace=True)\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_and_feature_engineering(df):\n",
    "    #drop one abruptly large value in AMT_PAYMENT_CURRENT\n",
    "    df.drop(df[df['AMT_PAYMENT_CURRENT'] > 4000000].index, inplace=True)\n",
    "\n",
    "    # feature engineering\n",
    "    # making the MONTHS_BALANCE Positive\n",
    "    df['MONTHS_BALANCE'] = np.abs(df['MONTHS_BALANCE'])\n",
    "\n",
    "    #sorting according to the month of status from oldest to latest, for rolling computations\n",
    "    df = df.sort_values(by = ['SK_ID_PREV','MONTHS_BALANCE'], ascending = [1,0])\n",
    "\n",
    "    df['AMT_DRAWINGS_SUM'] = df[['AMT_DRAWINGS_ATM_CURRENT', 'AMT_DRAWINGS_CURRENT', 'AMT_DRAWINGS_OTHER_CURRENT', 'AMT_DRAWINGS_POS_CURRENT']].sum(axis=1)\n",
    "    df['CNT_DRAWINGS_SUM'] = df[['CNT_DRAWINGS_ATM_CURRENT', 'CNT_DRAWINGS_CURRENT', 'CNT_DRAWINGS_OTHER_CURRENT', 'CNT_DRAWINGS_POS_CURRENT']].sum(axis=1)\n",
    "\n",
    "    df['BALANCE_LIMIT_RATIO'] = df['AMT_BALANCE'] / (df['AMT_CREDIT_LIMIT_ACTUAL'] + 0.00001)\n",
    "\n",
    "    df['MIN_PAYMENT_RATIO'] = df['AMT_PAYMENT_CURRENT'] / (df['AMT_INST_MIN_REGULARITY'] + 0.0001)\n",
    "    df['PAYMENT_MIN_DIFF'] = df['AMT_PAYMENT_CURRENT'] - df['AMT_INST_MIN_REGULARITY']\n",
    "    df['PAYMENT_MIN_DIFF'] = df['AMT_PAYMENT_TOTAL_CURRENT'] - df['AMT_INST_MIN_REGULARITY']\n",
    "    df['MIN_PAYMENT_TOTAL_RATIO'] = df['AMT_PAYMENT_TOTAL_CURRENT'] / (df['AMT_INST_MIN_REGULARITY'] +0.00001)\n",
    "\n",
    "    df['AMT_INTEREST_RECEIVABLE'] = df['AMT_TOTAL_RECEIVABLE'] - df['AMT_RECEIVABLE_PRINCIPAL']\n",
    "\n",
    "    df['SK_DPD_RATIO'] = df['SK_DPD'] / (df['SK_DPD_DEF'] + 0.00001)\n",
    "\n",
    "    # calculating the rolling Exponential Weighted Moving Average over months for certain features\n",
    "    rolling_col = [\n",
    "        'AMT_BALANCE',\n",
    "        'AMT_CREDIT_LIMIT_ACTUAL',\n",
    "        'AMT_RECEIVABLE_PRINCIPAL',\n",
    "        'AMT_RECIVABLE',\n",
    "        'AMT_TOTAL_RECEIVABLE',\n",
    "        'AMT_DRAWINGS_SUM',\n",
    "        'BALANCE_LIMIT_RATIO',\n",
    "        'CNT_DRAWINGS_SUM',\n",
    "        'MIN_PAYMENT_RATIO',\n",
    "        'PAYMENT_MIN_DIFF',\n",
    "        'MIN_PAYMENT_TOTAL_RATIO',\n",
    "        'AMT_INTEREST_RECEIVABLE',\n",
    "        'SK_DPD_RATIO']\n",
    "\n",
    "    exp_weighted_col = ['EXP_' + ele for ele in rolling_col]\n",
    "    df[exp_weighted_col] = df.groupby(['SK_ID_CURR','SK_ID_PREV'])[rolling_col].transform(lambda x: x.ewm(alpha = 0.7).mean())\n",
    "\n",
    "    #performing overall aggregations over SK_ID_PREV\n",
    "    overall_aggregations = {\n",
    "    'SK_ID_CURR' : ['first'],\n",
    "    'MONTHS_BALANCE': ['max'],\n",
    "    'AMT_BALANCE' : ['sum','mean','max'],\n",
    "    'AMT_CREDIT_LIMIT_ACTUAL' : ['sum','mean','max'],\n",
    "    'AMT_DRAWINGS_ATM_CURRENT' : ['sum','max'],\n",
    "    'AMT_DRAWINGS_CURRENT' : ['sum','max'],\n",
    "    'AMT_DRAWINGS_OTHER_CURRENT' : ['sum','max'],\n",
    "    'AMT_DRAWINGS_POS_CURRENT' : ['sum','max'],\n",
    "    'AMT_INST_MIN_REGULARITY' : ['mean','min','max'],\n",
    "    'AMT_PAYMENT_CURRENT' : ['mean','min','max'],\n",
    "    'AMT_PAYMENT_TOTAL_CURRENT' : ['mean','min','max'],\n",
    "    'AMT_RECEIVABLE_PRINCIPAL' : ['sum','mean','max'],\n",
    "    'AMT_RECIVABLE' : ['sum','mean','max'],\n",
    "    'AMT_TOTAL_RECEIVABLE' : ['sum','mean','max'],\n",
    "    'CNT_DRAWINGS_ATM_CURRENT' : ['sum','max'],\n",
    "    'CNT_DRAWINGS_CURRENT' : ['sum','max'],\n",
    "    'CNT_DRAWINGS_OTHER_CURRENT' : ['sum','max'],\n",
    "    'CNT_DRAWINGS_POS_CURRENT' : ['sum','max'],\n",
    "    'CNT_INSTALMENT_MATURE_CUM' : ['sum','max','min'],\n",
    "    'SK_DPD' : ['sum','max'],\n",
    "    'SK_DPD_DEF' : ['sum','max'],\n",
    "\n",
    "    'AMT_DRAWINGS_SUM' : ['sum','max'],\n",
    "    'BALANCE_LIMIT_RATIO' : ['mean','max','min'],\n",
    "    'CNT_DRAWINGS_SUM' : ['sum','max'],\n",
    "    'MIN_PAYMENT_RATIO': ['min','mean'],\n",
    "    'PAYMENT_MIN_DIFF' : ['min','mean'],\n",
    "    'MIN_PAYMENT_TOTAL_RATIO' : ['min','mean'],\n",
    "    'AMT_INTEREST_RECEIVABLE' : ['min','mean'],\n",
    "    'SK_DPD_RATIO' : ['max','mean'],\n",
    "\n",
    "    'EXP_AMT_BALANCE' : ['last'],\n",
    "    'EXP_AMT_CREDIT_LIMIT_ACTUAL' : ['last'],\n",
    "    'EXP_AMT_RECEIVABLE_PRINCIPAL' : ['last'],\n",
    "    'EXP_AMT_RECIVABLE' : ['last'],\n",
    "    'EXP_AMT_TOTAL_RECEIVABLE' : ['last'],\n",
    "    'EXP_AMT_DRAWINGS_SUM' : ['last'],\n",
    "    'EXP_BALANCE_LIMIT_RATIO' : ['last'],\n",
    "    'EXP_CNT_DRAWINGS_SUM' : ['last'],\n",
    "    'EXP_MIN_PAYMENT_RATIO' : ['last'],\n",
    "    'EXP_PAYMENT_MIN_DIFF' : ['last'],\n",
    "    'EXP_MIN_PAYMENT_TOTAL_RATIO' : ['last'],\n",
    "    'EXP_AMT_INTEREST_RECEIVABLE' : ['last'],\n",
    "    'EXP_SK_DPD_RATIO' : ['last'],\n",
    "    }\n",
    "\n",
    "    cc_balance_aggregated_overall = df.groupby('SK_ID_PREV').agg(overall_aggregations)\n",
    "    cc_balance_aggregated_overall.columns = ['_'.join(ele).upper() for ele in cc_balance_aggregated_overall.columns]\n",
    "    cc_balance_aggregated_overall.rename(columns = {'SK_ID_CURR_FIRST' : 'SK_ID_CURR'}, inplace = True)\n",
    "\n",
    "    # aggregating for different categories\n",
    "    aggregations_for_categories = {\n",
    "    'SK_DPD' : ['sum','max'],\n",
    "    'SK_DPD_DEF' : ['sum','max'],\n",
    "    'BALANCE_LIMIT_RATIO' : ['mean','max','min'],\n",
    "    'AMT_DRAWINGS_SUM' : ['sum','max'],\n",
    "    'CNT_DRAWINGS_SUM' : ['sum','max'],\n",
    "    'MIN_PAYMENT_RATIO': ['min','mean'],\n",
    "    'PAYMENT_MIN_DIFF' : ['min','mean'],\n",
    "    'MIN_PAYMENT_TOTAL_RATIO' : ['min','mean'],\n",
    "    'AMT_INTEREST_RECEIVABLE' : ['min','mean'],\n",
    "    'SK_DPD_RATIO' : ['max','mean'],\n",
    "    'EXP_AMT_DRAWINGS_SUM' : ['last'],\n",
    "    'EXP_BALANCE_LIMIT_RATIO' : ['last'],\n",
    "    'EXP_CNT_DRAWINGS_SUM' : ['last'],\n",
    "    'EXP_MIN_PAYMENT_RATIO' : ['last'],\n",
    "    'EXP_PAYMENT_MIN_DIFF' : ['last'],\n",
    "    'EXP_MIN_PAYMENT_TOTAL_RATIO' : ['last'],\n",
    "    'EXP_AMT_INTEREST_RECEIVABLE' : ['last'],\n",
    "    'EXP_SK_DPD_RATIO' : ['last']\n",
    "}\n",
    "\n",
    "    contract_status_categories = ['Active','Completed']\n",
    "    cc_balance_aggregated_categories = pd.DataFrame()\n",
    "    for i, contract_type in enumerate(contract_status_categories):\n",
    "        group = df[df['NAME_CONTRACT_STATUS'] == contract_type].groupby('SK_ID_PREV').agg(aggregations_for_categories)\n",
    "        group.columns = ['_'.join(ele).upper() + '_' + contract_type.upper() for ele in group.columns]\n",
    "        if i == 0:\n",
    "            cc_balance_aggregated_categories = group\n",
    "        else:\n",
    "            cc_balance_aggregated_categories = cc_balance_aggregated_categories.merge(group, on = 'SK_ID_PREV', how = 'outer')\n",
    "\n",
    "    # aggregating over SK_ID_PREV for rest of the categories\n",
    "    cc_balance_aggregated_categories_rest = df[(df['NAME_CONTRACT_STATUS'] != 'Active') &\n",
    "                                    (df.NAME_CONTRACT_STATUS != 'Completed')].groupby('SK_ID_PREV').agg(aggregations_for_categories)\n",
    "    cc_balance_aggregated_categories_rest.columns = ['_'.join(ele).upper() + '_REST' for ele in cc_balance_aggregated_categories_rest.columns]\n",
    "\n",
    "    #merging all the categorical aggregations\n",
    "    cc_balance_aggregated_categories = cc_balance_aggregated_categories.merge(cc_balance_aggregated_categories_rest, on = 'SK_ID_PREV', how = 'outer')\n",
    "\n",
    "    #merging all the aggregations\n",
    "    cc_aggregated = cc_balance_aggregated_overall.merge(cc_balance_aggregated_categories, on = 'SK_ID_PREV', how = 'outer')\n",
    "\n",
    "    #one-hot encoding the categorical column NAME_CONTRACT_STATUS\n",
    "    name_contract_dummies = pd.get_dummies(df.NAME_CONTRACT_STATUS, prefix='CONTRACT')\n",
    "    contract_names = name_contract_dummies.columns.tolist()\n",
    "\n",
    "    #merging the one-hot encoded feature with original table\n",
    "    df = pd.concat([df, name_contract_dummies], axis=1)\n",
    "    #aggregating over SK_ID_PREV the one-hot encoded columns\n",
    "    aggregated_cc_contract = df[['SK_ID_PREV'] + contract_names].groupby('SK_ID_PREV').mean()\n",
    "\n",
    "    #merging with the aggregated table\n",
    "    cc_aggregated = cc_aggregated.merge(aggregated_cc_contract, on = 'SK_ID_PREV', how = 'outer')\n",
    "\n",
    "    #aggregate on SK_ID_CURR\n",
    "    #from EDA, since most of the SK_ID_CURR had only 1 credit card, so for aggregations, we will take the means\n",
    "    cc_aggregated = cc_aggregated.groupby('SK_ID_CURR', as_index = False).mean()\n",
    "\n",
    "    return cc_aggregated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def credit_card_balance():\n",
    "    # Load data\n",
    "    cc = pd.read_csv('data/credit_card_balance.csv')\n",
    "    target = pd.read_csv('data/target.csv')\n",
    "\n",
    "    # Set target index\n",
    "    target.set_index('SK_ID_CURR', inplace=True)\n",
    "\n",
    "    # Merge with target and split train/test\n",
    "    cc_agg = cc.merge(target, on='SK_ID_CURR', how='left')\n",
    "    cc_train = cc_agg[~cc_agg['TARGET'].isnull()]\n",
    "    cc_test = cc_agg[cc_agg['TARGET'].isnull()]\n",
    "\n",
    "    # clean\n",
    "    # cc_train, cc_test = missing_values(cc_train, cc_test)\n",
    "    cc_train, cc_test = missing_values(cc_train, cc_test)\n",
    "    cc_train, cc_test = clipping_outliers_app(cc_train, cc_test)\n",
    "    # Process\n",
    "    cc_train_processed = preprocessing_and_feature_engineering(cc_train)\n",
    "    cc_train_processed = cc_train_processed.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    cc_test_processed = preprocessing_and_feature_engineering(cc_test)\n",
    "    cc_test_processed = cc_test_processed.replace([np.inf, -np.inf], np.nan)\n",
    "    # Combine train and test sets\n",
    "    processed_credit_card_balance = pd.concat([cc_train_processed, cc_test_processed], axis=0)\n",
    "    # print(11)\n",
    "    # train = processed_credit_card_balance[processed_credit_card_balance['TARGET'].notna()]\n",
    "    # test = processed_credit_card_balance[processed_credit_card_balance['TARGET'].isna()]\n",
    "    # train = select_feature(train)\n",
    "    # processed_credit_card_balance = pd.concat([train, test[train.columns]], axis=0)\n",
    "    print('\"Credit Card Balance\" final shape:', processed_credit_card_balance.shape)\n",
    "\n",
    "    return processed_credit_card_balance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEMPORARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_card_balance_temp():    \n",
    "    cc = pd.read_csv(r'data/credit_card_balance.csv')\n",
    "\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category=True)\n",
    "\n",
    "    # Amount used from limit\n",
    "    cc['LIMIT_USE'] = cc['AMT_BALANCE'] / cc['AMT_CREDIT_LIMIT_ACTUAL']\n",
    "    # Current payment / Min payment\n",
    "    cc['PAYMENT_DIV_MIN'] = cc['AMT_PAYMENT_CURRENT'] / cc['AMT_INST_MIN_REGULARITY']\n",
    "    # Late payment <-- 'CARD_IS_DPD'\n",
    "    cc['LATE_PAYMENT'] = cc['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    # How much drawing of limit\n",
    "    cc['DRAWING_LIMIT_RATIO'] = cc['AMT_DRAWINGS_ATM_CURRENT'] / cc['AMT_CREDIT_LIMIT_ACTUAL']\n",
    "\n",
    "    cc['CARD_IS_DPD_UNDER_120'] = cc['SK_DPD'].apply(lambda x: 1 if (x > 0) & (x < 120) else 0)\n",
    "    cc['CARD_IS_DPD_OVER_120'] = cc['SK_DPD'].apply(lambda x: 1 if x >= 120 else 0)\n",
    "\n",
    "    # General aggregations\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "\n",
    "    # Count credit card lines\n",
    "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "\n",
    "    # Last month balance of each credit card application\n",
    "    last_ids = cc.groupby('SK_ID_PREV')['MONTHS_BALANCE'].idxmax()\n",
    "    last_months_df = cc[cc.index.isin(last_ids)]\n",
    "    cc_agg = group_and_merge(last_months_df,cc_agg,'CC_LAST_', {'AMT_BALANCE': ['mean', 'max']})\n",
    "\n",
    "    CREDIT_CARD_TIME_AGG = {\n",
    "        'AMT_BALANCE': ['mean', 'max'],\n",
    "        'LIMIT_USE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_LIMIT_ACTUAL':['max'],\n",
    "        'AMT_DRAWINGS_ATM_CURRENT': ['max', 'sum'],\n",
    "        'AMT_DRAWINGS_CURRENT': ['max', 'sum'],\n",
    "        'AMT_DRAWINGS_POS_CURRENT': ['max', 'sum'],\n",
    "        'AMT_INST_MIN_REGULARITY': ['max', 'mean'],\n",
    "        'AMT_PAYMENT_TOTAL_CURRENT': ['max','sum'],\n",
    "        'AMT_TOTAL_RECEIVABLE': ['max', 'mean'],\n",
    "        'CNT_DRAWINGS_ATM_CURRENT': ['max','sum', 'mean'],\n",
    "        'CNT_DRAWINGS_CURRENT': ['max', 'mean', 'sum'],\n",
    "        'CNT_DRAWINGS_POS_CURRENT': ['mean'],\n",
    "        'SK_DPD': ['mean', 'max', 'sum'],\n",
    "        'LIMIT_USE': ['min', 'max'],\n",
    "        'DRAWING_LIMIT_RATIO': ['min', 'max'],\n",
    "        'LATE_PAYMENT': ['mean', 'sum'],\n",
    "        'CARD_IS_DPD_UNDER_120': ['mean', 'sum'],\n",
    "        'CARD_IS_DPD_OVER_120': ['mean', 'sum']\n",
    "    }\n",
    "\n",
    "    for months in [12, 24, 48]:\n",
    "        cc_prev_id = cc[cc['MONTHS_BALANCE'] >= -months]['SK_ID_PREV'].unique()\n",
    "        cc_recent = cc[cc['SK_ID_PREV'].isin(cc_prev_id)]\n",
    "        prefix = 'INS_{}M_'.format(months)\n",
    "        cc_agg = group_and_merge(cc_recent, cc_agg, prefix, CREDIT_CARD_TIME_AGG)\n",
    "\n",
    "\n",
    "    print('\"Credit Card Balance\" final shape:', cc_agg.shape)\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Installments Payments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinhHuong - fix feature engineering (add ewm), bổ sung aggregation\n",
    "\n",
    "def missing_values_ins(df_train, df_test):\n",
    "    # Remove null from AMT_INSTALMENT and SK_ID_CURR\n",
    "    df_train.dropna(subset=['SK_ID_CURR'], inplace = True)\n",
    "    df_test.dropna(subset=['SK_ID_CURR'], inplace = True)\n",
    "    df_train.dropna(subset=['AMT_INSTALMENT'], inplace = True)\n",
    "    df_test.dropna(subset=['AMT_INSTALMENT'], inplace = True)\n",
    "\n",
    "    # null in DAY_ENTRY_PAYMENT -> null in AMT_PAYMENT null: unpaid loan\n",
    "    max_days = df_train['DAYS_ENTRY_PAYMENT'].max() + 1\n",
    "    df_train['DAYS_ENTRY_PAYMENT'].fillna(max_days, inplace = True) # fill max value -> latest payment\n",
    "    df_test['DAYS_ENTRY_PAYMENT'].fillna(max_days, inplace = True)\n",
    "\n",
    "    df_train['AMT_PAYMENT'].fillna(0, inplace=True) # fill 0 -> unpaid\n",
    "    df_test['AMT_PAYMENT'].fillna(0, inplace=True)\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_ins(df):\n",
    "\n",
    "    # new features\n",
    "    df['AMT_PAYMENT_RATIO'] = df['AMT_PAYMENT'] / df['AMT_INSTALMENT']\n",
    "    df['FLAG_LOW_PAYMENT_RATIO'] = (df['AMT_PAYMENT_RATIO'] < 0.5).astype(int)\n",
    "\n",
    "    df['AMT_PAYMENT_DIFF'] = df['AMT_INSTALMENT'] - df['AMT_PAYMENT']\n",
    "    df['LESS_PAYMENT_FLAG'] = (df['AMT_PAYMENT'] < df['AMT_INSTALMENT']).astype(int)\n",
    "\n",
    "    df['DUE_FLAG'] = (df['DAYS_ENTRY_PAYMENT'] > df['DAYS_INSTALMENT']).astype(int)\n",
    "    df['DAYS_PAYMENT_RATIO'] = df['DAYS_INSTALMENT'] / df['DAYS_ENTRY_PAYMENT']\n",
    "    df['DAYS_PAYMENT_DIFF'] = df['DAYS_INSTALMENT'] - df['DAYS_ENTRY_PAYMENT']\n",
    "\n",
    "    # add ewm\n",
    "    df['EXP_DAYS_PAYMENT_RATIO'] = df['DAYS_PAYMENT_RATIO'].transform(lambda x: x.ewm(alpha = 0.5).mean())\n",
    "    df['EXP_DAYS_PAYMENT_DIFF'] = df['DAYS_PAYMENT_DIFF'].transform(lambda x: x.ewm(alpha = 0.5).mean())\n",
    "    df['EXP_AMT_PAYMENT_RATIO'] = df['AMT_PAYMENT_RATIO'].transform(lambda x: x.ewm(alpha = 0.5).mean())\n",
    "    df['EXP_AMT_PAYMENT_DIFF'] = df['AMT_PAYMENT_DIFF'].transform(lambda x: x.ewm(alpha = 0.5).mean())\n",
    "\n",
    "    # Aggregate features per SK_ID_PREV\n",
    "    aggregate = {\n",
    "    'NUM_INSTALMENT_VERSION' : ['mean','sum'],\n",
    "    'NUM_INSTALMENT_NUMBER' : ['max'],\n",
    "    'DAYS_INSTALMENT' : ['max','min'],\n",
    "    'DAYS_ENTRY_PAYMENT' : ['max','min'],\n",
    "    'AMT_INSTALMENT' : ['mean', 'sum', 'max'],\n",
    "    'AMT_PAYMENT' : ['mean', 'sum', 'max'],\n",
    "    'DAYS_PAYMENT_RATIO' : ['mean', 'min','max'],\n",
    "    'DAYS_PAYMENT_DIFF' : ['mean','min','max'],\n",
    "    'AMT_PAYMENT_RATIO' : ['mean','min','max'],\n",
    "    'AMT_PAYMENT_DIFF' : ['mean','min','max'],\n",
    "    'FLAG_LOW_PAYMENT_RATIO': ['mean', 'sum', 'last'],\n",
    "    'LESS_PAYMENT_FLAG': ['mean', 'sum'],\n",
    "    'DUE_FLAG': ['mean', 'sum'],\n",
    "    'EXP_DAYS_PAYMENT_RATIO' : ['last'],\n",
    "    'EXP_DAYS_PAYMENT_DIFF' : ['last'],\n",
    "    'EXP_AMT_PAYMENT_RATIO' : ['last'],\n",
    "    'EXP_AMT_PAYMENT_DIFF' : ['last']\n",
    "    }\n",
    "\n",
    "    limited_period_aggregations = {\n",
    "    'NUM_INSTALMENT_VERSION' : ['mean','sum'],\n",
    "    'AMT_PAYMENT_RATIO': ['mean', 'min'],\n",
    "    'AMT_PAYMENT_DIFF': ['mean', 'sum'],\n",
    "    'LESS_PAYMENT_FLAG': ['mean', 'sum'],\n",
    "    'DUE_FLAG': ['mean', 'sum'],\n",
    "    'DAYS_PAYMENT_RATIO': ['mean', 'max'],\n",
    "    'DAYS_PAYMENT_DIFF': ['mean', 'sum'],\n",
    "    'EXP_DAYS_PAYMENT_RATIO' : ['last'],\n",
    "    'EXP_DAYS_PAYMENT_DIFF' : ['last'],\n",
    "    'EXP_AMT_PAYMENT_RATIO' : ['last'],\n",
    "    'EXP_AMT_PAYMENT_DIFF' : ['last']\n",
    "    }A\n",
    "\n",
    "    special_aggregations = {\n",
    "    'AMT_PAYMENT_RATIO': ['mean', 'min'],\n",
    "    'AMT_PAYMENT_DIFF': ['mean', 'sum'],\n",
    "    'LESS_PAYMENT_FLAG': ['mean', 'sum'],\n",
    "    'DUE_FLAG': ['mean', 'sum'],\n",
    "    'DAYS_PAYMENT_RATIO': ['mean', 'max'],\n",
    "    'DAYS_PAYMENT_DIFF': ['mean', 'sum'],\n",
    "    'EXP_DAYS_PAYMENT_RATIO' : ['last'],\n",
    "    'EXP_DAYS_PAYMENT_DIFF' : ['last'],\n",
    "    'EXP_AMT_PAYMENT_RATIO' : ['last'],\n",
    "    'EXP_AMT_PAYMENT_DIFF' : ['last']}\n",
    "\n",
    "    # Aggregates over last 60, 90, 180 & 365 days\n",
    "    time = {'60': -60, '90': -90, '180': -180, '365': -365}\n",
    "    aggregations_time = []\n",
    "\n",
    "    for key, value in time.items():\n",
    "        target = df[df['DAYS_INSTALMENT'] > value]\n",
    "        time_agg = target.groupby('SK_ID_PREV').agg(limited_period_aggregations)\n",
    "        time_agg.columns = ['_'.join(ele).upper() + f'_LAST_{key}_DAYS' for ele in time_agg.columns]\n",
    "        aggregations_time.append(time_agg)\n",
    "\n",
    "    aggregations_time = pd.concat(aggregations_time, axis=1)\n",
    "\n",
    "    # First 5 installments aggregation\n",
    "    group_first_5_instalments = df.groupby('SK_ID_PREV', as_index = False).head(5).groupby('SK_ID_PREV').agg(limited_period_aggregations)\n",
    "    group_first_5_instalments.columns = ['_'.join(ele).upper() + '_FIRST_5_INSTALLMENTS' for ele in group_first_5_instalments.columns]\n",
    "\n",
    "    #Aggregates over NUM_INSTALMENT_NUMBER at 4 installments\n",
    "    instalment_4 = df[df['NUM_INSTALMENT_NUMBER'] == 4]\n",
    "    aggregations_instalment_4 = instalment_4.groupby('SK_ID_PREV').agg(special_aggregations)\n",
    "    aggregations_instalment_4.columns = ['_'.join(col).upper() + '_INSTAL_4' for col in aggregations_instalment_4.columns]\n",
    "\n",
    "    # aggregates over all installments that were past due\n",
    "    past_due = df[df['DUE_FLAG'] == 1]\n",
    "    aggregations_past_due = past_due.groupby('SK_ID_PREV').agg(special_aggregations)\n",
    "    aggregations_past_due.columns = ['_'.join(col).upper() + '_PAST_DUE' for col in aggregations_past_due.columns]\n",
    "\n",
    "    #overall aggregation of installments_payments over SK_ID_PREV\n",
    "    group_overall = df.groupby(['SK_ID_PREV','SK_ID_CURR'], as_index = False).agg(aggregate)\n",
    "    group_overall.columns = ['_'.join(ele).upper() for ele in group_overall.columns]\n",
    "    group_overall.rename(columns = {'SK_ID_PREV_': 'SK_ID_PREV','SK_ID_CURR_' : 'SK_ID_CURR'}, inplace = True)\n",
    "\n",
    "    # merge\n",
    "    installments_payments_agg_prev = group_overall.merge(aggregations_time, on = 'SK_ID_PREV', how = 'outer')\n",
    "    installments_payments_agg_prev = installments_payments_agg_prev.merge(group_first_5_instalments, on = 'SK_ID_PREV', how = 'outer')\n",
    "    installments_payments_agg_prev = installments_payments_agg_prev.merge(aggregations_instalment_4, on = 'SK_ID_PREV', how = 'outer')\n",
    "    installments_payments_agg_prev = installments_payments_agg_prev.merge(aggregations_past_due, on = 'SK_ID_PREV', how = 'outer')\n",
    "\n",
    "    #aggregating over SK_ID_CURR\n",
    "    main_features_aggregations = {\n",
    "    'NUM_INSTALMENT_VERSION_MEAN' : ['mean'],\n",
    "    'NUM_INSTALMENT_VERSION_SUM' : ['mean'],\n",
    "    'NUM_INSTALMENT_NUMBER_MAX' : ['mean','sum','max'],\n",
    "    'AMT_INSTALMENT_MEAN' : ['mean','sum','max'],\n",
    "    'AMT_INSTALMENT_SUM' : ['mean','sum','max'],\n",
    "    'AMT_INSTALMENT_MAX' : ['mean'],\n",
    "    'AMT_PAYMENT_MEAN' : ['mean','sum','max'],\n",
    "    'AMT_PAYMENT_SUM' : ['mean','sum','max'],\n",
    "    'AMT_PAYMENT_MAX' : ['mean'],\n",
    "    'DAYS_PAYMENT_RATIO_MEAN' : ['mean','min','max'],\n",
    "    'DAYS_PAYMENT_RATIO_MIN' : ['mean','min'],\n",
    "    'DAYS_PAYMENT_RATIO_MAX' : ['mean','max'],\n",
    "    'DAYS_PAYMENT_DIFF_MEAN' : ['mean','min','max'],\n",
    "    'DAYS_PAYMENT_DIFF_MIN' : ['mean','min'],\n",
    "    'DAYS_PAYMENT_DIFF_MAX' : ['mean','max'],\n",
    "    'AMT_PAYMENT_RATIO_MEAN' : ['mean', 'min','max'],\n",
    "    'AMT_PAYMENT_RATIO_MIN' : ['mean','min'],\n",
    "    'AMT_PAYMENT_RATIO_MAX' : ['mean','max'],\n",
    "    'AMT_PAYMENT_DIFF_MEAN' : ['mean','min','max'],\n",
    "    'AMT_PAYMENT_DIFF_MIN' : ['mean','min'],\n",
    "    'AMT_PAYMENT_DIFF_MAX' : ['mean','max'],\n",
    "    'EXP_DAYS_PAYMENT_RATIO_LAST' : ['mean'],\n",
    "    'EXP_DAYS_PAYMENT_DIFF_LAST' : ['mean'],\n",
    "    'EXP_AMT_PAYMENT_RATIO_LAST' : ['mean'],\n",
    "    'EXP_AMT_PAYMENT_DIFF_LAST' : ['mean']\n",
    "    }\n",
    "\n",
    "    grouped_main_features = installments_payments_agg_prev.groupby('SK_ID_CURR').agg(main_features_aggregations)\n",
    "    grouped_main_features.columns = ['_'.join(ele).upper() for ele in grouped_main_features.columns]\n",
    "\n",
    "    #group remaining ones\n",
    "    remaining_columns = [col for col in installments_payments_agg_prev.columns if col not in grouped_main_features.columns and col != 'SK_ID_CURR']\n",
    "    grouped_remaining_features = installments_payments_agg_prev[['SK_ID_CURR'] + remaining_columns].groupby('SK_ID_CURR').mean()\n",
    "\n",
    "    df_agg = grouped_main_features.merge(grouped_remaining_features, on = 'SK_ID_CURR', how = 'inner')\n",
    "    df_agg = df_agg.reset_index()\n",
    "\n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def installments_payments():\n",
    "    # Load data\n",
    "    instal = pd.read_csv('data/installments_payments.csv')\n",
    "    target = pd.read_csv('data/target.csv')\n",
    "\n",
    "    # Set target index\n",
    "    target.set_index('SK_ID_CURR', inplace=True)\n",
    "\n",
    "    # Merge with target and split train/test\n",
    "    instal_agg = instal.merge(target, on='SK_ID_CURR', how='left')\n",
    "    instal_train = instal_agg[~instal_agg['TARGET'].isnull()]\n",
    "    instal_test = instal_agg[instal_agg['TARGET'].isnull()]\n",
    "\n",
    "    # clean\n",
    "    # instal_train, instal_test = missing_values(instal_train, instal_test)\n",
    "    instal_train, instal_test = missing_values_ins(instal_train, instal_test)\n",
    "    instal_train, instal_test = clipping_outliers_app(instal_train, instal_test)\n",
    "    # Process\n",
    "    instal_train_processed = feature_engineering_ins(instal_train)\n",
    "    instal_train_processed = instal_train_processed.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    instal_test_processed = feature_engineering_ins(instal_test)\n",
    "    instal_test_processed = instal_test_processed.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # Combine train and test sets\n",
    "    processed_installments_payments = pd.concat([instal_train_processed, instal_test_processed], axis=0)\n",
    "    # train = processed_installments_payments[processed_installments_payments['TARGET'].notna()]\n",
    "    # test = processed_installments_payments[processed_installments_payments['TARGET'].isna()]\n",
    "    # train = select_feature(train)\n",
    "    # processed_installments_payments = pd.concat([train, test[train.columns]], axis=0)\n",
    "    print('\"Installments Payments\" final shape:', processed_installments_payments.shape)\n",
    "    return processed_installments_payments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEMPORARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def installments_payments_temp():\n",
    "    ins = pd.read_csv(r'data/installments_payments.csv')\n",
    "\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category=True)\n",
    "\n",
    "    # Group payments and get Payment difference\n",
    "    ins = do_sum(ins, ['SK_ID_PREV', 'NUM_INSTALMENT_NUMBER'], 'AMT_PAYMENT', 'AMT_PAYMENT_GROUPED')\n",
    "    ins['PAYMENT_DIFFERENCE'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT_GROUPED']\n",
    "    ins['PAYMENT_RATIO'] = ins['AMT_INSTALMENT'] / ins['AMT_PAYMENT_GROUPED']\n",
    "    ins['PAID_OVER_AMOUNT'] = ins['AMT_PAYMENT'] - ins['AMT_INSTALMENT']\n",
    "    ins['PAID_OVER'] = (ins['PAID_OVER_AMOUNT'] > 0).astype(int)\n",
    "\n",
    "    # Percentage and difference paid in each installment (amount paid and installment value)\n",
    "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "\n",
    "    # Days past due and days before due (no negative values)\n",
    "    ins['DPD_diff'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "    ins['DBD_diff'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "    ins['DPD'] = ins['DPD_diff'].apply(lambda x: x if x > 0 else 0)\n",
    "    ins['DBD'] = ins['DBD_diff'].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "    # Flag late payment\n",
    "    ins['LATE_PAYMENT'] = ins['DBD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    ins['INSTALMENT_PAYMENT_RATIO'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    ins['LATE_PAYMENT_RATIO'] = ins.apply(lambda x: x['INSTALMENT_PAYMENT_RATIO'] if x['LATE_PAYMENT'] == 1 else 0, axis=1)\n",
    "\n",
    "    # Flag late payments that have a significant amount\n",
    "    ins['SIGNIFICANT_LATE_PAYMENT'] = ins['LATE_PAYMENT_RATIO'].apply(lambda x: 1 if x > 0.05 else 0)\n",
    "    \n",
    "    # Flag k threshold late payments\n",
    "    ins['DPD_7'] = ins['DPD'].apply(lambda x: 1 if x >= 7 else 0)\n",
    "    ins['DPD_15'] = ins['DPD'].apply(lambda x: 1 if x >= 15 else 0)\n",
    "\n",
    "    ins['INS_IS_DPD_UNDER_120'] = ins['DPD'].apply(lambda x: 1 if (x > 0) & (x < 120) else 0)\n",
    "    ins['INS_IS_DPD_OVER_120'] = ins['DPD'].apply(lambda x: 1 if (x >= 120) else 0)\n",
    "\n",
    "    # Features: Perform aggregations\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum', 'var'],\n",
    "        'DBD': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum', 'min'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum', 'min'],\n",
    "        'SK_ID_PREV': ['size', 'nunique'],\n",
    "        'PAYMENT_DIFFERENCE': ['mean'],\n",
    "        'PAYMENT_RATIO': ['mean', 'max'],\n",
    "        'LATE_PAYMENT': ['mean', 'sum'],\n",
    "        'SIGNIFICANT_LATE_PAYMENT': ['mean', 'sum'],\n",
    "        'LATE_PAYMENT_RATIO': ['mean'],\n",
    "        'DPD_7': ['mean'],\n",
    "        'DPD_15': ['mean'],\n",
    "        'PAID_OVER': ['mean'],\n",
    "        'DPD_diff':['mean', 'min', 'max'],\n",
    "        'DBD_diff':['mean', 'min', 'max'],\n",
    "        'DAYS_INSTALMENT': ['mean', 'max', 'sum'],\n",
    "        'INS_IS_DPD_UNDER_120': ['mean', 'sum'],\n",
    "        'INS_IS_DPD_OVER_120': ['mean', 'sum']\n",
    "    }\n",
    "\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "\n",
    "    # Count installments accounts\n",
    "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "\n",
    "    # from oof (DAYS_ENTRY_PAYMENT)\n",
    "    cond_day = ins['DAYS_ENTRY_PAYMENT'] >= -365\n",
    "    ins_d365_grp = ins[cond_day].groupby('SK_ID_CURR')\n",
    "    ins_d365_agg_dict = {\n",
    "        'SK_ID_CURR': ['count'],\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['mean', 'max', 'sum'],\n",
    "        'DAYS_INSTALMENT': ['mean', 'max', 'sum'],\n",
    "        'AMT_INSTALMENT': ['mean', 'max', 'sum'],\n",
    "        'AMT_PAYMENT': ['mean', 'max', 'sum'],\n",
    "        'PAYMENT_DIFF': ['mean', 'min', 'max', 'sum'],\n",
    "        'PAYMENT_PERC': ['mean', 'max'],\n",
    "        'DPD_diff': ['mean', 'min', 'max'],\n",
    "        'DPD': ['mean', 'sum'],\n",
    "        'INS_IS_DPD_UNDER_120': ['mean', 'sum'],\n",
    "        'INS_IS_DPD_OVER_120': ['mean', 'sum']}\n",
    "\n",
    "    ins_d365_agg = ins_d365_grp.agg(ins_d365_agg_dict)\n",
    "    ins_d365_agg.columns = ['INS_D365' + ('_').join(column).upper() for column in ins_d365_agg.columns.ravel()]\n",
    "\n",
    "    ins_agg = ins_agg.merge(ins_d365_agg, on='SK_ID_CURR', how='left')\n",
    "    ins_agg.reset_index(inplace=True)\n",
    "    print('\"Installments Payments\" final shape:', ins_agg.shape)\n",
    "    return ins_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Pos Cash Balance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder_pos(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = df.select_dtypes(include='object').columns.tolist()\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "def do_sum_pos(dataframe, group_cols, counted, agg_name):\n",
    "    gp = dataframe[group_cols + [counted]].groupby(group_cols)[counted].sum().reset_index().rename(columns={counted: agg_name})\n",
    "    dataframe = dataframe.merge(gp, on=group_cols, how='left')\n",
    "    return dataframe\n",
    "\n",
    "# convert days to years, remove invalid value, drop col with large number of missing value\n",
    "def prep_fill_missing_pos(dataframe):\n",
    "    # Replace specific invalid values with NaN\n",
    "    values_to_replace = ['XNA', 'XAP']\n",
    "    dataframe.replace(values_to_replace, np.nan, inplace=True)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "def lazy_fill_missing_pos(dataframe_train, dataframe_test):\n",
    "    col_num = dataframe_train.select_dtypes('number').columns.drop('TARGET')\n",
    "    for col in col_num:\n",
    "        median_value = dataframe_train[col].median()\n",
    "        dataframe_train[col] = dataframe_train[col].fillna(median_value)\n",
    "        dataframe_test[col] = dataframe_test[col].fillna(median_value)\n",
    "    col_object = dataframe_train.select_dtypes('object').columns\n",
    "    for col in col_object:\n",
    "        mode_value = dataframe_train[col].mode()[0]\n",
    "        dataframe_train[col] = dataframe_train[col].fillna(mode_value)\n",
    "        dataframe_test[col] = dataframe_test[col].fillna(mode_value)\n",
    "    return dataframe_train, dataframe_test\n",
    "\n",
    "# fill by mean (for numeric column) or by mode (for category column) by each specified column\n",
    "def fill_missing_pos(dataframe_train, dataframe_test):\n",
    "\n",
    "    # FILL BY 'AMT_APPLICATION'\n",
    "    dataframe_train['MONTH_BALANCE_BINS'] = pd.cut(dataframe_train['MONTHS_BALANCE'], 10)\n",
    "    dataframe_test['MONTH_BALANCE_BINS'] = pd.cut(dataframe_test['MONTHS_BALANCE'], 10)\n",
    "\n",
    "    col_to_group = 'MONTH_BALANCE_BINS'\n",
    "    col_to_impute = ['CNT_INSTALMENT_FUTURE', 'CNT_INSTALMENT']\n",
    "    # vòng for dùng để impute cho nhiều cột\n",
    "    for i, col in enumerate(col_to_impute):\n",
    "        # sau khi group kiểu này, grouped chứa 2 phần tử: val_to_group (giá trị được group, ở đây là từng Age_bin) và group_data (dataframe có Age_bin = val_to_group)\n",
    "        grouped = dataframe_train.groupby([col_to_group])[col]\n",
    "        for val_to_group, group_data in grouped:\n",
    "            if dataframe_train[col].dtypes in [int, float]:  # fill biến numeric = mean của từng group_data\n",
    "                mean_value = group_data.mean()\n",
    "                # phải dùng val_to_group[0] là bởi val_to_group được trả về là tuple do hàm cut ban đầu (đừng quan tâm cái này vội)\n",
    "                dataframe_train.loc[(dataframe_train[col_to_group] == val_to_group[0]) & (dataframe_train[col].isna()), col] = mean_value\n",
    "                dataframe_test.loc[(dataframe_test[col_to_group] == val_to_group[0]) & (dataframe_test[col].isna()), col] = mean_value\n",
    "            else:  # fill biến câte = mode của từng group_data\n",
    "                mode_value = group_data.mode().iloc[0] if not group_data.mode().empty else np.nan\n",
    "                dataframe_train.loc[(dataframe_train[col_to_group] == val_to_group[0]) & (dataframe_train[col].isna()), col] = mode_value\n",
    "                dataframe_test.loc[(dataframe_test[col_to_group] == val_to_group[0]) & (dataframe_test[col].isna()), col] = mode_value\n",
    "    dataframe_train.drop(columns='MONTH_BALANCE_BINS', inplace=True)\n",
    "    dataframe_test.drop(columns='MONTH_BALANCE_BINS', inplace=True)\n",
    "    return dataframe_train, dataframe_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banr moi.\n",
    "def create_feature_pos(df):\n",
    "    # Drop duplicates for cleaning\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # One-hot encoding of categorical variables\n",
    "    df, cat_cols = one_hot_encoder_pos(df, nan_as_category=True)\n",
    "\n",
    "    # Feature engineering for late payment behavior\n",
    "    df['LATE_PAYMENT'] = df['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    df['POS_IS_DPD'] = df['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    df['POS_IS_DPD_UNDER_120'] = df['SK_DPD'].apply(lambda x: 1 if (x > 0) & (x < 120) else 0)\n",
    "    df['POS_IS_DPD_OVER_120'] = df['SK_DPD'].apply(lambda x: 1 if x >= 120 else 0)\n",
    "\n",
    "    # Weighted payment efficiency (based on time) cai nao gan ngay hon, thi weight nang hon\n",
    "    df['DAYS_WEIGHT'] = 1 / (df['MONTHS_BALANCE'] + 1)  # Avoid division by zero\n",
    "    df['WEIGHTED_PAYMENT_EFFICIENCY'] = df['SK_DPD'] * df['DAYS_WEIGHT']\n",
    "\n",
    "    # Aggregations for key metrics\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size', 'min'],\n",
    "        'SK_DPD': ['max', 'mean', 'sum', 'var', 'min'],\n",
    "        'SK_DPD_DEF': ['max', 'mean', 'sum'],\n",
    "        'SK_ID_PREV': ['nunique'],\n",
    "        'LATE_PAYMENT': ['mean'],\n",
    "        'CNT_INSTALMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'CNT_INSTALMENT_FUTURE': ['min', 'max', 'mean', 'sum'],\n",
    "        'POS_IS_DPD': ['mean', 'sum'],\n",
    "        'POS_IS_DPD_UNDER_120': ['mean', 'sum'],\n",
    "        'POS_IS_DPD_OVER_120': ['mean', 'sum'],\n",
    "        'WEIGHTED_PAYMENT_EFFICIENCY': ['mean', 'sum']\n",
    "    }\n",
    "\n",
    "    # Include categorical aggregations\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "\n",
    "    # Perform aggregation\n",
    "    df_agg = df.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    df_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n",
    "    df_agg['POS_COUNT'] = df.groupby('SK_ID_CURR').size()\n",
    "\n",
    "    # # Add trend features from the most recent applications\n",
    "    # last_month_df = df.groupby('SK_ID_PREV')['MONTHS_BALANCE'].idxmax()\n",
    "    # last_month_records = df.loc[last_month_df]\n",
    "    # recent_gp = (\n",
    "    #     last_month_records\n",
    "    #     .sort_values(by=['SK_ID_CURR', 'MONTHS_BALANCE'], ascending=[True, False])\n",
    "    #     .groupby('SK_ID_CURR')\n",
    "    #     .head(3)\n",
    "    # )\n",
    "\n",
    "    # recent_gp_agg = recent_gp.groupby('SK_ID_CURR').mean().reset_index()\n",
    "    # df_agg = pd.merge(df_agg, recent_gp_agg[['SK_ID_CURR', 'LATE_PAYMENT']], on='SK_ID_CURR', how='left', suffixes=('', '_LAST3'))\n",
    "\n",
    "    # # Add percentage of late payments for last 3 applications\n",
    "    # df_agg['POS_LATE_PAYMENT_RATIO_LAST3'] = df_agg['LATE_PAYMENT_LAST3'] / 3\n",
    "        # Find the most recent 3 records for each SK_ID_CURR\n",
    "    last_month_df = df.groupby('SK_ID_PREV')['MONTHS_BALANCE'].idxmax()  # Most recent month per previous ID\n",
    "    last_month_records = df.loc[last_month_df]  # Subset of most recent records\n",
    "    gp = (\n",
    "        last_month_records\n",
    "        .sort_values(by=['SK_ID_CURR', 'MONTHS_BALANCE'], ascending=[True, False])  # Sort by recency\n",
    "        .groupby('SK_ID_CURR')\n",
    "        .head(3)  # Top 3 most recent applications\n",
    "    )\n",
    "\n",
    "    # Calculate the mean of LATE_PAYMENT for these 3 applications\n",
    "    late_payment_last3 = gp.groupby('SK_ID_CURR')['LATE_PAYMENT'].mean().reset_index()\n",
    "    late_payment_last3.rename(columns={'LATE_PAYMENT': 'LATE_PAYMENT_LAST3'}, inplace=True)\n",
    "\n",
    "    # Merge this feature back into the aggregated DataFrame\n",
    "    df_agg = pd.merge(df_agg, late_payment_last3, on='SK_ID_CURR', how='left')\n",
    "\n",
    "    print('\"Pos-Cash\" balance final shape:', df_agg.shape)\n",
    "    return df_agg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_cash():\n",
    "    \"\"\"Process the POS_CASH_balance data by merging, transforming, and scaling.\"\"\"\n",
    "    # Load data\n",
    "    pos = pd.read_csv('data/POS_CASH_balance.csv')\n",
    "    app_train = pd.read_csv('data/application_train.csv')\n",
    "    app_test = pd.read_csv('data/application_test.csv')\n",
    "    app_full = pd.concat([app_train, app_test], axis=0)\n",
    "    target = app_full[['SK_ID_CURR','TARGET']]\n",
    "    target.set_index('SK_ID_CURR', inplace=True)\n",
    "    pos = pos.merge(target, on='SK_ID_CURR', how='inner')\n",
    "    # split train and test\n",
    "    train = pos[~pos['TARGET'].isnull()]\n",
    "    test = pos[pos['TARGET'].isnull()]\n",
    "\n",
    "    #prepare before filling missing\n",
    "    train = prep_fill_missing_pos(train)\n",
    "    test = prep_fill_missing_pos(test)\n",
    "     # fill missing\n",
    "    train, test = fill_missing_pos(train, test)\n",
    "    # train, test = lazy_fill_missing_pos(train, test)\n",
    "    train, test = clipping_outliers_app(train, test)\n",
    "    #FE\n",
    "    train = create_feature_pos(train)\n",
    "    test = create_feature_pos(test)\n",
    "    train = train.merge(target, on='SK_ID_CURR', how='left')\n",
    "    train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    test = test.merge(target, on='SK_ID_CURR', how='left')\n",
    "    test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Combine train and test back after cleaning\n",
    "    train = select_feature(train)\n",
    "    df = pd.concat([train, test[train.columns]], axis=0)\n",
    "    df.drop(columns='TARGET', inplace=True)\n",
    "    print('\"Application_Train_Test\" final shape:', df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEMPORARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_cash_temp():\n",
    "    pos = pd.read_csv(r'data/POS_CASH_balance.csv')\n",
    "\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category=True)\n",
    "\n",
    "    # Flag months with late payment\n",
    "    pos['LATE_PAYMENT'] = pos['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    pos['POS_IS_DPD'] = pos['SK_DPD'].apply(lambda x: 1 if x > 0 else 0) # <-- same with ['LATE_PAYMENT']\n",
    "    pos['POS_IS_DPD_UNDER_120'] = pos['SK_DPD'].apply(lambda x: 1 if (x > 0) & (x < 120) else 0)\n",
    "    pos['POS_IS_DPD_OVER_120'] = pos['SK_DPD'].apply(lambda x: 1 if x >= 120 else 0)\n",
    "\n",
    "    # Features\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size', 'min'],\n",
    "        'SK_DPD': ['max', 'mean', 'sum', 'var', 'min'],\n",
    "        'SK_DPD_DEF': ['max', 'mean', 'sum'],\n",
    "        'SK_ID_PREV': ['nunique'],\n",
    "        'LATE_PAYMENT': ['mean'],\n",
    "        'SK_ID_CURR': ['count'],\n",
    "        'CNT_INSTALMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'CNT_INSTALMENT_FUTURE': ['min', 'max', 'mean', 'sum'],\n",
    "        'POS_IS_DPD': ['mean', 'sum'],\n",
    "        'POS_IS_DPD_UNDER_120': ['mean', 'sum'],\n",
    "        'POS_IS_DPD_OVER_120': ['mean', 'sum'],\n",
    "    }\n",
    "\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "\n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "\n",
    "    # Count pos cash accounts\n",
    "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "\n",
    "\n",
    "    sort_pos = pos.sort_values(by=['SK_ID_PREV', 'MONTHS_BALANCE'])\n",
    "    gp = sort_pos.groupby('SK_ID_PREV')\n",
    "    df_pos = pd.DataFrame()\n",
    "    df_pos['SK_ID_CURR'] = gp['SK_ID_CURR'].first()\n",
    "    df_pos['MONTHS_BALANCE_MAX'] = gp['MONTHS_BALANCE'].max()\n",
    "\n",
    "    # Percentage of previous loans completed and completed before initial term\n",
    "    df_pos['POS_LOAN_COMPLETED_MEAN'] = gp['NAME_CONTRACT_STATUS_Completed'].mean()\n",
    "    df_pos['POS_COMPLETED_BEFORE_MEAN'] = gp['CNT_INSTALMENT'].first() - gp['CNT_INSTALMENT'].last()\n",
    "    df_pos['POS_COMPLETED_BEFORE_MEAN'] = df_pos.apply(lambda x: 1 if x['POS_COMPLETED_BEFORE_MEAN'] > 0 \\\n",
    "                                                                      and x['POS_LOAN_COMPLETED_MEAN'] > 0 else 0, axis=1)\n",
    "    # Number of remaining installments (future installments) and percentage from total\n",
    "    df_pos['POS_REMAINING_INSTALMENTS'] = gp['CNT_INSTALMENT_FUTURE'].last()\n",
    "    df_pos['POS_REMAINING_INSTALMENTS_RATIO'] = gp['CNT_INSTALMENT_FUTURE'].last()/gp['CNT_INSTALMENT'].last()\n",
    "\n",
    "    # Group by SK_ID_CURR and merge\n",
    "    df_gp = df_pos.groupby('SK_ID_CURR').sum().reset_index()\n",
    "    df_gp.drop(['MONTHS_BALANCE_MAX'], axis=1, inplace= True)\n",
    "    pos_agg = pd.merge(pos_agg, df_gp, on= 'SK_ID_CURR', how= 'left')\n",
    "\n",
    "    # Percentage of late payments for the 3 most recent applications\n",
    "    pos = do_sum(pos, ['SK_ID_PREV'], 'LATE_PAYMENT', 'LATE_PAYMENT_SUM')\n",
    "\n",
    "    # Last month of each application\n",
    "    last_month_df = pos.groupby('SK_ID_PREV')['MONTHS_BALANCE'].idxmax()\n",
    "\n",
    "    # Most recent applications (last 3)\n",
    "    sort_pos = pos.sort_values(by=['SK_ID_PREV', 'MONTHS_BALANCE'])\n",
    "    gp = sort_pos.iloc[last_month_df].groupby('SK_ID_CURR').tail(3)\n",
    "    gp_mean = gp.groupby('SK_ID_CURR').mean().reset_index()\n",
    "    pos_agg = pd.merge(pos_agg, gp_mean[['SK_ID_CURR', 'LATE_PAYMENT_SUM']], on='SK_ID_CURR', how='left')\n",
    "\n",
    "    print('\"Pos-Cash\" balance final shape:', pos_agg.shape) \n",
    "    return pos_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos = pos_cash()\n",
    "# pos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Previous Application "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help function\n",
    "def one_hot_encoder_prev_app(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = df.select_dtypes(include='object').columns.tolist()\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "\n",
    "# convert days to years, remove invalid value, drop col with large number of missing value\n",
    "def prep_fill_missing_prev_app(dataframe):\n",
    "\n",
    "    # convert days to years\n",
    "    def days_to_year(dataframe):\n",
    "        for col in dataframe.columns[dataframe.columns.str.startswith('DAYS')]:\n",
    "            dataframe[f'YEARS{col[4:]}'] = np.ceil(dataframe[col] / -365)\n",
    "\n",
    "        return dataframe\n",
    "    dataframe = days_to_year(dataframe)\n",
    "\n",
    "    # delete invalid in years (< 0)\n",
    "    dataframe[['YEARS_DECISION', 'YEARS_FIRST_DRAWING', 'YEARS_FIRST_DUE', 'YEARS_LAST_DUE_1ST_VERSION', 'YEARS_LAST_DUE', 'YEARS_TERMINATION']].applymap(lambda x: np.nan if x < 0 else x)\n",
    "    # delete invalid in days (> 0)\n",
    "    dataframe[['DAYS_DECISION', 'DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE', 'DAYS_LAST_DUE_1ST_VERSION', 'DAYS_LAST_DUE', 'DAYS_TERMINATION']] = dataframe[['DAYS_DECISION', 'DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE', 'DAYS_LAST_DUE_1ST_VERSION', 'DAYS_LAST_DUE', 'DAYS_TERMINATION']].applymap(lambda x: np.nan if x > 0 else x)\n",
    "\n",
    "    # drop >95% missing\n",
    "    col_to_drop = ['RATE_INTEREST_PRIMARY', 'RATE_INTEREST_PRIVILEGED']\n",
    "    dataframe.drop(columns=col_to_drop, inplace=True)\n",
    "\n",
    "    # Replace specific invalid DAYS values with NaN\n",
    "    cols_to_replace_nan = ['DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE',\n",
    "                           'DAYS_LAST_DUE_1ST_VERSION', 'DAYS_LAST_DUE', 'DAYS_TERMINATION']\n",
    "    for col in cols_to_replace_nan:\n",
    "        if col in dataframe.columns:\n",
    "            dataframe[col].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "    # Replace specific invalid values with NaN\n",
    "    values_to_replace = ['XNA', 'XAP']\n",
    "    dataframe.replace(values_to_replace, np.nan, inplace=True)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "def lazy_fill_missing_prev(dataframe_train, dataframe_test):\n",
    "    col_num = dataframe_train.select_dtypes('number').columns.drop('TARGET')\n",
    "    for col in col_num:\n",
    "        median_value = dataframe_train[col].median()\n",
    "        dataframe_train[col] = dataframe_train[col].fillna(median_value)\n",
    "        dataframe_test[col] = dataframe_test[col].fillna(median_value)\n",
    "    col_object = dataframe_train.select_dtypes('object').columns\n",
    "    for col in col_object:\n",
    "        mode_value = dataframe_train[col].mode()[0]\n",
    "        dataframe_train[col] = dataframe_train[col].fillna(mode_value)\n",
    "        dataframe_test[col] = dataframe_test[col].fillna(mode_value)\n",
    "    return dataframe_train, dataframe_test\n",
    "\n",
    "# fill by mean (for numeric column) or by mode (for category column) by each specified column\n",
    "def fill_missing_prev_app(dataframe_train, dataframe_test):\n",
    "    # NAME_TYPE_SUITE: ng di cung khi vay fill = mode\n",
    "    # mode_suite = dataframe_train['NAME_TYPE_SUITE'].mode()\n",
    "    # dataframe_train['NAME_TYPE_SUITE'].fillna(mode_suite, inplace=True)\n",
    "    # dataframe_test['NAME_TYPE_SUITE'].fillna(mode_suite, inplace=True)\n",
    "\n",
    "    # PRODUCT_COMBINATION\n",
    "    mode_combination = dataframe_train['PRODUCT_COMBINATION'].mode()\n",
    "    dataframe_train['PRODUCT_COMBINATION'].fillna(mode_combination, inplace=True)\n",
    "    dataframe_test['PRODUCT_COMBINATION'].fillna(mode_combination, inplace=True)\n",
    "\n",
    "    # FILL BY 'AMT_APPLICATION'\n",
    "    dataframe_train['AMT_APP_BINS'] = pd.cut(dataframe_train['AMT_APPLICATION'], 10)\n",
    "    dataframe_test['AMT_APP_BINS'] = pd.cut(dataframe_test['AMT_APPLICATION'], 10)\n",
    "\n",
    "    col_to_group = 'AMT_APP_BINS'\n",
    "    col_to_impute = ['AMT_GOODS_PRICE', 'AMT_ANNUITY','AMT_DOWN_PAYMENT','RATE_DOWN_PAYMENT', 'AMT_CREDIT','NAME_TYPE_SUITE']\n",
    "    # vòng for dùng để impute cho nhiều cột\n",
    "    for i, col in enumerate(col_to_impute):\n",
    "        # sau khi group kiểu này, grouped chứa 2 phần tử: val_to_group (giá trị được group, ở đây là từng Age_bin) và group_data (dataframe có Age_bin = val_to_group)\n",
    "        grouped = dataframe_train.groupby([col_to_group])[col]\n",
    "        for val_to_group, group_data in grouped:\n",
    "            if dataframe_train[col].dtypes in [int, float]:  # fill biến numeric = mean của từng group_data\n",
    "                mean_value = group_data.mean()\n",
    "                # phải dùng val_to_group[0] là bởi val_to_group được trả về là tuple do hàm cut ban đầu (đừng quan tâm cái này vội)\n",
    "                dataframe_train.loc[(dataframe_train[col_to_group] == val_to_group[0]) & (dataframe_train[col].isna()), col] = mean_value\n",
    "                dataframe_test.loc[(dataframe_test[col_to_group] == val_to_group[0]) & (dataframe_test[col].isna()), col] = mean_value\n",
    "            else:  # fill biến câte = mode của từng group_data\n",
    "                mode_value = group_data.mode().iloc[0] if not group_data.mode().empty else np.nan\n",
    "                dataframe_train.loc[(dataframe_train[col_to_group] == val_to_group[0]) & (dataframe_train[col].isna()), col] = mode_value\n",
    "                dataframe_test.loc[(dataframe_test[col_to_group] == val_to_group[0]) & (dataframe_test[col].isna()), col] = mode_value\n",
    "    dataframe_train.drop(columns='AMT_APP_BINS', inplace=True)\n",
    "    dataframe_test.drop(columns='AMT_APP_BINS', inplace=True)\n",
    "    return dataframe_train, dataframe_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_prev_app(df):\n",
    "    # Preserve the original PRODUCT_COMBINATION column\n",
    "    product_combination = df[['SK_ID_CURR', 'DAYS_DECISION', 'PRODUCT_COMBINATION']].copy()\n",
    "\n",
    "    # Apply one-hot encoding\n",
    "    df, cat_cols = one_hot_encoder_prev_app(df, nan_as_category=True)\n",
    "\n",
    "    # Restore the PRODUCT_COMBINATION column\n",
    "    df = df.merge(product_combination, on=['SK_ID_CURR', 'DAYS_DECISION'], how='left')\n",
    "\n",
    "    # Ratios and Differences\n",
    "    df['APP_CREDIT_PERC'] = df['AMT_APPLICATION'] / df['AMT_CREDIT']\n",
    "    df['APPLICATION_CREDIT_DIFF'] = df['AMT_APPLICATION'] - df['AMT_CREDIT']\n",
    "    df['CREDIT_TO_ANNUITY_RATIO'] = df['AMT_CREDIT'] / df['AMT_ANNUITY']\n",
    "    df['DOWN_PAYMENT_TO_CREDIT'] = df['AMT_DOWN_PAYMENT'] / df['AMT_CREDIT']\n",
    "    df['SIMPLE_INTERESTS'] = (df['AMT_ANNUITY'] * df['CNT_PAYMENT'] / df['AMT_CREDIT'] - 1) / df['CNT_PAYMENT']\n",
    "    df['DAYS_LAST_DUE_DIFF'] = df['DAYS_LAST_DUE_1ST_VERSION'] - df['DAYS_LAST_DUE']\n",
    "    df['PREV_GOODS_DIFF'] = df['AMT_APPLICATION'] - df['AMT_GOODS_PRICE']\n",
    "    df['PREV_ANNUITY_APPL_RATIO'] = df['AMT_ANNUITY'] / df['AMT_APPLICATION']\n",
    "    df['PREV_GOODS_APPL_RATIO'] = df['AMT_GOODS_PRICE'] / df['AMT_APPLICATION']\n",
    "\n",
    "    # Add prev_PRODUCT_COMBINATION: Most recent PRODUCT_COMBINATION\n",
    "    df['RECENT_PRODUCT_COMBINATION'] = df.sort_values('DAYS_DECISION').groupby('SK_ID_CURR')['PRODUCT_COMBINATION'].transform('last')\n",
    "\n",
    "    # Aggregations for numeric features\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean', 'sum'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean', 'sum'],\n",
    "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean', 'sum'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "        'SK_ID_PREV': ['nunique'],\n",
    "        'DAYS_TERMINATION': ['max'],\n",
    "        'CREDIT_TO_ANNUITY_RATIO': ['mean', 'max'],\n",
    "        'APPLICATION_CREDIT_DIFF': ['min', 'max', 'mean', 'sum'],\n",
    "        'DOWN_PAYMENT_TO_CREDIT': ['mean'],\n",
    "        'PREV_GOODS_DIFF': ['mean', 'max', 'sum'],\n",
    "        'PREV_GOODS_APPL_RATIO': ['mean', 'max'],\n",
    "        'DAYS_LAST_DUE_DIFF': ['mean', 'max', 'sum'],\n",
    "        'SIMPLE_INTERESTS': ['mean', 'max']\n",
    "    }\n",
    "\n",
    "    # Categorical feature aggregation\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "\n",
    "    # Aggregating for all previous applications\n",
    "    df_agg = df.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    df_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n",
    "\n",
    "    # Add PRODUCT_COMBINATION\n",
    "    recent_combination = df.groupby('SK_ID_CURR')['RECENT_PRODUCT_COMBINATION'].last()\n",
    "    df_agg['PREV_RECENT_PRODUCT_COMBINATION'] = recent_combination\n",
    "\n",
    "    # Aggregate Last 3, 5 and First 2, 4 Applications\n",
    "    def aggregate_subsets(df, n_first, n_last):\n",
    "        first_n = df.groupby('SK_ID_CURR').head(n_first).groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "        first_n.columns = [f'FIRST{n_first}_' + e[0] + \"_\" + e[1].upper() for e in first_n.columns]\n",
    "\n",
    "        last_n = df.groupby('SK_ID_CURR').tail(n_last).groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "        last_n.columns = [f'LAST{n_last}_' + e[0] + \"_\" + e[1].upper() for e in last_n.columns]\n",
    "\n",
    "        return first_n, last_n\n",
    "\n",
    "    for n_first, n_last in [(2, 3), (4, 5)]:\n",
    "        first_n, last_n = aggregate_subsets(df, n_first, n_last)\n",
    "        df_agg = df_agg.join(first_n, how='left', on='SK_ID_CURR')\n",
    "        df_agg = df_agg.join(last_n, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    # Approved and Refused Applications Aggregation\n",
    "    for status, prefix in zip(['Approved', 'Refused'], ['APPROVED', 'REFUSED']):\n",
    "        subset = df[df[f'NAME_CONTRACT_STATUS_{status}'] == 1]\n",
    "        subset_agg = subset.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "        subset_agg.columns = pd.Index([f'{prefix}_' + e[0] + \"_\" + e[1].upper() for e in subset_agg.columns.tolist()])\n",
    "        df_agg = df_agg.join(subset_agg, how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_application():\n",
    "  data = pd.read_csv('data/previous_application.csv')\n",
    "  app_train = pd.read_csv('data/application_train.csv')\n",
    "  app_test = pd.read_csv('data/application_test.csv')\n",
    "  app_full = pd.concat([app_train, app_test], axis=0)\n",
    "  target = app_full[['SK_ID_CURR','TARGET']]\n",
    "  target.set_index('SK_ID_CURR', inplace=True)\n",
    "  data = data.merge(target, on='SK_ID_CURR', how='inner')\n",
    "\n",
    "  # split train and test\n",
    "  train = data[~data['TARGET'].isnull()]\n",
    "  test = data[data['TARGET'].isnull()]\n",
    "\n",
    "  #prepare before filling missing\n",
    "  train = prep_fill_missing_prev_app(train)\n",
    "  test = prep_fill_missing_prev_app(test)\n",
    "\n",
    "  # fill missing\n",
    "  # train, test = fill_missing_prev_app(train, test)\n",
    "  train, test = fill_missing_prev_app(train, test)\n",
    "  train, test = clipping_outliers_app(train, test)\n",
    "  #FE\n",
    "  train = create_feature_prev_app(train)\n",
    "  test = create_feature_prev_app(test)\n",
    "  train = train.merge(target, on='SK_ID_CURR', how='left').reset_index()\n",
    "  train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "  test = test.merge(target, on='SK_ID_CURR', how='left').reset_index()\n",
    "  test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "  # Combine train and test back after cleaning\n",
    "  train = select_feature(train)\n",
    "  df = pd.concat([train, test[train.columns]], axis=0)\n",
    "  df.drop(columns='TARGET', inplace=True)\n",
    "\n",
    "  df, cat_cols = one_hot_encoder_prev_app(df, nan_as_category=True)\n",
    "\n",
    "  print('Previous Application final shape:', df.shape)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEMPORARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_application_temp():\n",
    "    prev = pd.read_csv('data/previous_application.csv')\n",
    "\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category=True)\n",
    "\n",
    "    # Days 365.243 values -> nan\n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "    # Add feature: value ask / value received percentage\n",
    "    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "\n",
    "    # Feature engineering: ratios and difference\n",
    "    prev['APPLICATION_CREDIT_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_CREDIT']\n",
    "    prev['CREDIT_TO_ANNUITY_RATIO'] = prev['AMT_CREDIT'] / prev['AMT_ANNUITY']\n",
    "    prev['DOWN_PAYMENT_TO_CREDIT'] = prev['AMT_DOWN_PAYMENT'] / prev['AMT_CREDIT']\n",
    "\n",
    "    # Interest ratio on previous application (simplified)\n",
    "    total_payment = prev['AMT_ANNUITY'] * prev['CNT_PAYMENT']\n",
    "    prev['SIMPLE_INTERESTS'] = (total_payment / prev['AMT_CREDIT'] - 1) / prev['CNT_PAYMENT']\n",
    "\n",
    "    # Days last due difference (scheduled x done)\n",
    "    prev['DAYS_LAST_DUE_DIFF'] = prev['DAYS_LAST_DUE_1ST_VERSION'] - prev['DAYS_LAST_DUE']\n",
    "\n",
    "    # from off\n",
    "    prev['PREV_GOODS_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_GOODS_PRICE']\n",
    "    prev['PREV_ANNUITY_APPL_RATIO'] = prev['AMT_ANNUITY']/prev['AMT_APPLICATION']\n",
    "    prev['PREV_GOODS_APPL_RATIO'] = prev['AMT_GOODS_PRICE'] / prev['AMT_APPLICATION']\n",
    "\n",
    "    # Previous applications numeric features\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean', 'sum'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean', 'sum'],\n",
    "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean', 'sum'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "        'SK_ID_PREV': ['nunique'],\n",
    "        'DAYS_TERMINATION': ['max'],\n",
    "        'CREDIT_TO_ANNUITY_RATIO': ['mean', 'max'],\n",
    "        'APPLICATION_CREDIT_DIFF': ['min', 'max', 'mean', 'sum'],\n",
    "        'DOWN_PAYMENT_TO_CREDIT': ['mean'],\n",
    "        'PREV_GOODS_DIFF': ['mean', 'max', 'sum'],\n",
    "        'PREV_GOODS_APPL_RATIO': ['mean', 'max'],\n",
    "        'DAYS_LAST_DUE_DIFF': ['mean', 'max', 'sum'],\n",
    "        'SIMPLE_INTERESTS': ['mean', 'max']\n",
    "    }\n",
    "\n",
    "    # Previous applications categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "\n",
    "    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "\n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "    prev_agg.reset_index(inplace=True)\n",
    "    print('\"Previous Applications\" final shape:', prev_agg.shape)\n",
    "    return prev_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, train, test, target, *dfs):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.target = target\n",
    "        self.dfs = dfs\n",
    "        \n",
    "        self.features = None\n",
    "        self.imputer = SimpleImputer(strategy='mean').set_output(transform='pandas')\n",
    "        self.scaler = StandardScaler().set_output(transform='pandas')\n",
    "\n",
    "    def process(self, train, test, target):\n",
    "        # Set index and sort dataframes\n",
    "        train.set_index('SK_ID_CURR', inplace=True)\n",
    "        train.sort_index(inplace=True)\n",
    "        test.set_index('SK_ID_CURR', inplace=True)\n",
    "        test.sort_index(inplace=True)\n",
    "        target.set_index('SK_ID_CURR', inplace=True)\n",
    "        target.sort_index(inplace=True)\n",
    "        target = target['TARGET']\n",
    "\n",
    "        # Add is_train column and merge train and test data\n",
    "        train['is_train'] = 1\n",
    "        test['is_train'] = 0\n",
    "        data = pd.concat([train, test], axis=0)\n",
    "\n",
    "        # Merge additional dataframes \n",
    "        for df in self.dfs:\n",
    "            data = data.merge(df, how='left', on='SK_ID_CURR')\n",
    "        # data['annuity_to_max_installment_ratio'] = data['AMT_ANNUITY'] / data['INSTAL_AMT_PAYMENT_MAX']\n",
    "        # Remove duplicated columns, replace infinite values with NaN, and drop TARGET column\n",
    "        data = data.loc[:, ~data.columns.duplicated()]\n",
    "        data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        data.drop(['TARGET'], axis=1, inplace=True, errors='ignore')\n",
    "        data.set_index('SK_ID_CURR', inplace=True)\n",
    "        print(f'Merged data shape: {data.shape}')\n",
    "        # Split train and test data\n",
    "        train = data[data['is_train'] == 1].drop(['is_train'], axis=1)\n",
    "        test = data[data['is_train'] == 0].drop(['is_train'], axis=1)\n",
    "\n",
    "        # # Sanitize columns and convert data types to float64\n",
    "        # train = sanitize_columns(train)\n",
    "        # test = sanitize_columns(test)\n",
    "        # print(train.shape)\n",
    "        train = train.astype('float64')\n",
    "        test = test.astype('float64')\n",
    "        return train, test, target\n",
    "\n",
    "    def transform(self, data):\n",
    "        data = self.imputer.transform(data)\n",
    "        data = self.scaler.transform(data)\n",
    "        return data\n",
    "    \n",
    "    def fit_transform(self, train, target):\n",
    "        train = self.imputer.fit_transform(train)\n",
    "        train = self.scaler.fit_transform(train)\n",
    "        return train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Credit Card Balance\" final shape: (85322, 168)\n",
      "Time for credit card : 70.86s\n",
      "\"Installments Payments\" final shape: (180733, 211)\n",
      "Time for installments : 27.37s\n",
      "\"Pos-Cash\" balance final shape: (231531, 37)\n",
      "\"Pos-Cash\" balance final shape: (57913, 37)\n",
      "keep: 11 \n",
      "drop iv: 20 \n",
      "drop empty: 0 \n",
      "drop corr: 7\n",
      "\"Application_Train_Test\" final shape: (289444, 10)\n",
      "Time for posh cash : 35.95s\n",
      "keep: 93 \n",
      "drop iv: 220 \n",
      "drop empty: 54 \n",
      "drop corr: 97\n",
      "Previous Application final shape: (291057, 101)\n",
      "Time for previous application : 134.75s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "app_train_test = application()\n",
    "end = time.time()\n",
    "print(f'Time for application : {np.round(end - start, 2)}s')\n",
    "\n",
    "\n",
    "# # disable since second run\n",
    "start = time.time()\n",
    "# bureau_temp = bureau_and_balance_temp() \n",
    "bureau = bureau_and_balance()\n",
    "end = time.time()\n",
    "print(f'Time for bureau_bb : {np.round(end - start, 2)}s')\n",
    "\n",
    "start = time.time()\n",
    "# credit_card_temp = credit_card_balance_temp() \n",
    "credit_card = credit_card_balance() \n",
    "end = time.time()\n",
    "print(f'Time for credit card : {np.round(end - start, 2)}s')\n",
    "\n",
    "start = time.time()\n",
    "# ins_temp = installments_payments_temp() \n",
    "ins = installments_payments() \n",
    "end = time.time()\n",
    "print(f'Time for installments : {np.round(end - start, 2)}s')\n",
    "\n",
    "start = time.time()\n",
    "# pos_cash_balance = pos_cash_temp() \n",
    "pos_cash_balance = pos_cash() \n",
    "end = time.time()\n",
    "print(f'Time for posh cash : {np.round(end - start, 2)}s')\n",
    "\n",
    "start = time.time()\n",
    "# prev_app_temp = previous_application_temp() \n",
    "prev_app = previous_application() \n",
    "end = time.time()\n",
    "print(f'Time for previous application : {np.round(end - start, 2)}s')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app_train : (246009, 59)\n",
      "app_test : (61502, 59)\n",
      "target : (246009, 2)\n",
      "previous_application : (291057, 101)\n",
      "credit_card_balance : (85322, 168)\n",
      "installments_payments : (180733, 211)\n",
      "bureau : (263491, 52)\n",
      "pos_cash_balance : (289444, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "app_train = app_train_test[app_train_test['TARGET'].notna()]\n",
    "app_test= app_train_test[app_train_test['TARGET'].isna()]\n",
    "target = app_train_test[['SK_ID_CURR', 'TARGET']].dropna()\n",
    "\n",
    "print(f'app_train : {app_train.shape}')\n",
    "print(f'app_test : {app_test.shape}')\n",
    "print(f'target : {target.shape}')\n",
    "\n",
    "print(f'previous_application : {prev_app.shape}')\n",
    "print(f'credit_card_balance : {credit_card.shape}')\n",
    "print(f'installments_payments : {ins.shape}')\n",
    "print(f'bureau : {bureau.shape}')\n",
    "print(f'pos_cash_balance : {pos_cash_balance.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data shape: (307511, 595)\n",
      "Processing time: 23.90s\n",
      "train : (246009, 565)\n",
      "target : (246009,)\n",
      "test : (61502, 565)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# Initialize DataProcessor object\n",
    "processor = DataProcessor(app_train, app_test, target,\n",
    "                          prev_app,credit_card, \n",
    "                          ins, bureau, pos_cash_balance)\n",
    "\n",
    "# Process train and test data, fit and transform train data, and transform test data\n",
    "train, test, target = processor.process(app_train, app_test, target)\n",
    "train = processor.fit_transform(train, target)\n",
    "test = processor.transform(test)\n",
    "end = time.time()\n",
    "print(f\"Processing time: {end - start:.2f}s\")\n",
    "print(f'train : {train.shape}')\n",
    "print(f'target : {target.shape}')\n",
    "print(f'test : {test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validating and calculating metrics...\n",
      "ROC AUC: 0.7690999369017111\n",
      "Precision: 0.1729006440317639\n",
      "Recall: 0.6956127993560073\n",
      "F1-score: 0.2769603669835038\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.71      0.82    226133\n",
      "         1.0       0.17      0.70      0.28     19876\n",
      "\n",
      "    accuracy                           0.71    246009\n",
      "   macro avg       0.57      0.70      0.55    246009\n",
      "weighted avg       0.90      0.71      0.77    246009\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_logis = LogisticRegression(penalty='l2', class_weight='balanced', C=0.001, solver='newton-cholesky', max_iter=1000)\n",
    "# Cross-validation predictions\n",
    "print('Cross validating and calculating metrics...')\n",
    "y_pred_proba = cross_val_predict(model_logis, train, target, cv=5, method='predict_proba')[:, 1]  # Predicted probabilities\n",
    "y_pred = cross_val_predict(model_logis, train, target, cv=5, method='predict')  # Predicted classes\n",
    "\n",
    "# Calculate metrics\n",
    "roc_auc = roc_auc_score(target, y_pred_proba)\n",
    "precision = precision_score(target, y_pred)\n",
    "recall = recall_score(target, y_pred)\n",
    "f1 = f1_score(target, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f'ROC AUC: {roc_auc}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1-score: {f1}')\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(target, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model_logis\n",
    "# y_pred = model.predict_proba(test)[:, 1]\n",
    "# submission = pd.DataFrame(index=test.index, data={'TARGET': y_pred})\n",
    "# submission.sort_index(inplace=True)\n",
    "\n",
    "# submission.to_csv(f'submit/submit_34_final_{np.round(gini_score, 4)}.csv')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
